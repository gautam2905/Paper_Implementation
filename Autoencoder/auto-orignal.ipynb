{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614eecfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb9373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a59ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbcc74e8",
   "metadata": {},
   "source": [
    "# Gemini Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db432fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "defmse_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Squared Error loss.\"\"\"\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "defcross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"Cross-entropy loss for binary data or logistic outputs.\n",
    "    Assumes y_true are probabilities and y_pred are reconstructed probabilities.\n",
    "    From the paper: [-sum_i p_i log(p_hat_i) - sum_i (1-p_i)log(1-p_hat_i)] [cite: 112]\n",
    "    or [-sum_i p_i log(p_hat_i)] for multiclass cross-entropy [cite: 127]\n",
    "    This example uses a common binary cross-entropy form.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-7 # to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "class RBM:\n",
    "    \"\"\"\n",
    "    A Restricted Boltzmann Machine (RBM).\n",
    "    This class implements the core logic for an RBM layer,\n",
    "    which will be used for pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate=0.01, n_epochs=100, batch_size=32, k=1):\n",
    "        \"\"\"\n",
    "        Initialize RBM parameters.\n",
    "        n_visible: Number of visible units\n",
    "        n_hidden: Number of hidden units\n",
    "        learning_rate: Learning rate for weight updates\n",
    "        n_epochs: Number of training epochs\n",
    "        batch_size: Size of mini-batches for training\n",
    "        k: Number of Gibbs sampling steps for Contrastive Divergence (CD-k)\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.k = k # Number of steps in Contrastive Divergence\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        # Weights are typically initialized with small random values\n",
    "        self.W = np.random.randn(n_visible, n_hidden) * 0.1\n",
    "        self.h_bias = np.zeros(n_hidden) # Hidden layer biases [cite: 79]\n",
    "        self.v_bias = np.zeros(n_visible) # Visible layer biases [cite: 79]\n",
    "\n",
    "    def _sample_h_given_v(self, v):\n",
    "        \"\"\"\n",
    "        Sample hidden layer activations given visible layer states.\n",
    "        P(h_j=1|v) = sigmoid(b_j + sum_i v_i * w_ij) [cite: 84]\n",
    "        v: visible layer states (batch_size, n_visible)\n",
    "        Returns:\n",
    "            h_prob: hidden layer activation probabilities\n",
    "            h_sample: sampled binary hidden layer states\n",
    "        \"\"\"\n",
    "        h_prob = sigmoid(np.dot(v, self.W) + self.h_bias)\n",
    "        h_sample = (h_prob > np.random.rand(h_prob.shape[0], h_prob.shape[1])).astype(np.float32)\n",
    "        return h_prob, h_sample\n",
    "\n",
    "    def _sample_v_given_h(self, h):\n",
    "        \"\"\"\n",
    "        Sample visible layer activations (reconstruction) given hidden layer states.\n",
    "        P(v_i=1|h) = sigmoid(b_i + sum_j h_j * w_ij) [cite: 85]\n",
    "        (For Gaussian visible units, this would sample from a Gaussian distribution [cite: 104])\n",
    "        h: hidden layer states (batch_size, n_hidden)\n",
    "        Returns:\n",
    "            v_prob: visible layer activation probabilities (reconstruction)\n",
    "            v_sample: sampled binary visible layer states\n",
    "        \"\"\"\n",
    "        v_prob = sigmoid(np.dot(h, self.W.T) + self.v_bias)\n",
    "        v_sample = (v_prob > np.random.rand(v_prob.shape[0], v_prob.shape[1])).astype(np.float32)\n",
    "        return v_prob, v_sample\n",
    "\n",
    "    def contrastive_divergence(self, v_batch):\n",
    "        \"\"\"\n",
    "        Perform one step of Contrastive Divergence (CD-k).\n",
    "        v_batch: A mini-batch of visible data (batch_size, n_visible)\n",
    "        \"\"\"\n",
    "        # Positive phase: data-driven\n",
    "        # <v_i h_j>_data [cite: 89]\n",
    "        h_prob_orig, h_sample_orig = self._sample_h_given_v(v_batch)\n",
    "        \n",
    "        # Negative phase: reconstruction-driven (Gibbs sampling)\n",
    "        # <v_i h_j>_recon [cite: 89]\n",
    "        v_sample_recon = None\n",
    "        h_sample_recon = h_sample_orig\n",
    "        for step in range(self.k):\n",
    "            v_prob_recon, v_sample_recon = self._sample_v_given_h(h_sample_recon)\n",
    "            h_prob_recon, h_sample_recon = self._sample_h_given_v(v_sample_recon)\n",
    "\n",
    "        # Calculate gradients\n",
    "        # delta_w_ij = epsilon * (<v_i h_j>_data - <v_i h_j>_recon) [cite: 89]\n",
    "        positive_grad = np.dot(v_batch.T, h_prob_orig) # Using probabilities for smoother gradient\n",
    "        negative_grad = np.dot(v_prob_recon.T, h_prob_recon) # Using probabilities for smoother gradient\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.W += self.learning_rate * (positive_grad - negative_grad) / v_batch.shape[0]\n",
    "        self.h_bias += self.learning_rate * np.mean(h_prob_orig - h_prob_recon, axis=0)\n",
    "        self.v_bias += self.learning_rate * np.mean(v_batch - v_prob_recon, axis=0) # v_batch for original, v_prob_recon for reconstruction\n",
    "\n",
    "        # Reconstruction error for monitoring\n",
    "        error = np.mean((v_batch - v_prob_recon)**2)\n",
    "        return error\n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"\n",
    "        Train the RBM.\n",
    "        data: Training data (n_samples, n_visible)\n",
    "        \"\"\"\n",
    "        print(f\"Training RBM: {self.n_visible} -> {self.n_hidden}\")\n",
    "        num_batches = int(np.ceil(data.shape[0] / self.batch_size))\n",
    "        for epoch in range(self.n_epochs):\n",
    "            epoch_error = 0\n",
    "            # Shuffle data\n",
    "            np.random.shuffle(data)\n",
    "            for i in range(num_batches):\n",
    "                batch_data = data[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                if batch_data.shape[0] == 0:\n",
    "                    continue\n",
    "                error = self.contrastive_divergence(batch_data)\n",
    "                epoch_error += error\n",
    "            \n",
    "            print(f\"  Epoch {epoch+1}/{self.n_epochs}, Reconstruction Error: {epoch_error/num_batches:.4f}\")\n",
    "        print(f\"RBM training complete for {self.n_visible} -> {self.n_hidden}.\\n\")\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Transform data by computing hidden layer activations.\n",
    "        This is used as input for the next RBM in the stack. [cite: 70]\n",
    "        data: Input data (n_samples, n_visible)\n",
    "        Returns:\n",
    "            hidden_activations: Probabilities of hidden unit activations\n",
    "        \"\"\"\n",
    "        h_prob, _ = self._sample_h_given_v(data)\n",
    "        # \"While training higher level RBMs, the visible units\n",
    "        # were set to the activation probabilities of the\n",
    "        # hidden units in the previous RBM\" [cite: 106]\n",
    "        return h_prob\n",
    "\n",
    "\n",
    "class DeepAutoencoder:\n",
    "    \"\"\"\n",
    "    A Deep Autoencoder network.\n",
    "    Can be initialized with pretrained RBM weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01, n_epochs_finetune=50, batch_size=32, use_cross_entropy_loss=False):\n",
    "        \"\"\"\n",
    "        Initialize Deep Autoencoder.\n",
    "        layer_sizes: List of integers specifying the number of neurons in each layer,\n",
    "                     e.g., [784, 400, 200, 50] means 784 input, 2 hidden layers, 50 code layer.\n",
    "                     The decoder will be symmetric.\n",
    "        learning_rate: Learning rate for fine-tuning.\n",
    "        n_epochs_finetune: Number of epochs for fine-tuning.\n",
    "        batch_size: Mini-batch size for fine-tuning.\n",
    "        use_cross_entropy_loss: Boolean, if True use cross-entropy, else MSE.\n",
    "                                The paper mentions cross-entropy for logistic outputs. [cite: 112]\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs_finetune = n_epochs_finetune\n",
    "        self.batch_size = batch_size\n",
    "        self.use_cross_entropy_loss = use_cross_entropy_loss\n",
    "\n",
    "        self.encoder_weights = [] # List to store weights for encoder layers W1, W2, ...\n",
    "        self.encoder_biases = []  # List to store biases for encoder layers\n",
    "        self.decoder_weights = [] # List to store weights for decoder layers W_L^T, ... W1^T\n",
    "        self.decoder_biases = []  # List to store biases for decoder layers\n",
    "\n",
    "        # Note: For simplicity, this example uses sigmoid for all hidden layers.\n",
    "        # The paper mentions code layers can be linear. [cite: 114, 122, 128]\n",
    "        # And input/output layers might be linear with Gaussian noise or logistic. [cite: 104, 112, 105]\n",
    "\n",
    "    def initialize_weights_from_rbms(self, rbm_stack):\n",
    "        \"\"\"\n",
    "        Initialize encoder and decoder weights and biases from a stack of trained RBMs.\n",
    "        This is the \"unrolling\" phase. [cite: 71]\n",
    "        rbm_stack: A list of trained RBM objects.\n",
    "        \"\"\"\n",
    "        print(\"Unrolling RBM weights into Deep Autoencoder...\")\n",
    "        if len(rbm_stack) != (self.n_layers - 1):\n",
    "            raise ValueError(\"Number of RBMs must match number of autoencoder encoding layers.\")\n",
    "\n",
    "        for i in range(self.n_layers - 1): # For each encoding layer\n",
    "            rbm = rbm_stack[i]\n",
    "            self.encoder_weights.append(np.copy(rbm.W)) # W_i\n",
    "            self.encoder_biases.append(np.copy(rbm.h_bias)) # bias for hidden layer of RBM_i\n",
    "\n",
    "            # Decoder weights are transposes of encoder weights initially [cite: 102]\n",
    "            # Decoder biases are the visible biases of the corresponding RBM\n",
    "            self.decoder_weights.insert(0, np.copy(rbm.W.T)) # W_i^T\n",
    "            self.decoder_biases.insert(0, np.copy(rbm.v_bias)) # bias for visible layer of RBM_i\n",
    "        print(\"Autoencoder weights initialized from RBMs.\\n\")\n",
    "        \n",
    "    def _initialize_random_weights(self):\n",
    "        \"\"\"Initialize weights randomly if not pretraining.\"\"\"\n",
    "        print(\"Initializing Autoencoder weights randomly...\")\n",
    "        for i in range(self.n_layers - 1):\n",
    "            # Encoder\n",
    "            w_enc = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * 0.1\n",
    "            b_enc = np.zeros(self.layer_sizes[i+1])\n",
    "            self.encoder_weights.append(w_enc)\n",
    "            self.encoder_biases.append(b_enc)\n",
    "\n",
    "            # Decoder (symmetric structure)\n",
    "            w_dec = np.random.randn(self.layer_sizes[i+1], self.layer_sizes[i]) * 0.1\n",
    "            b_dec = np.zeros(self.layer_sizes[i])\n",
    "            self.decoder_weights.insert(0, w_dec) # Add to beginning for reverse order\n",
    "            self.decoder_biases.insert(0, b_dec)\n",
    "        print(\"Autoencoder weights initialized randomly.\\n\")\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encoder pass: data -> code\n",
    "        x: input data (batch_size, n_features)\n",
    "        Returns:\n",
    "            activations: list of activations at each encoder layer (including code layer)\n",
    "        \"\"\"\n",
    "        a = x\n",
    "        activations = [a]\n",
    "        for i in range(len(self.encoder_weights)):\n",
    "            # The paper mentions the code layer can be linear [cite: 114]\n",
    "            # For simplicity, we use sigmoid for all layers here.\n",
    "            # A more complete implementation would allow specifying activation per layer.\n",
    "            z = np.dot(a, self.encoder_weights[i]) + self.encoder_biases[i]\n",
    "            a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "        return activations # Last element is the code\n",
    "\n",
    "    def decode(self, code_activations_list):\n",
    "        \"\"\"\n",
    "        Decoder pass: code -> reconstruction\n",
    "        code_activations_list: list of activations from the encoder pass,\n",
    "                               needed for backpropagation. The actual code is the last element.\n",
    "        Returns:\n",
    "            reconstruction: reconstructed data\n",
    "            decoder_activations: list of activations at each decoder layer (for backprop)\n",
    "        \"\"\"\n",
    "        a = code_activations_list[-1] # This is the code layer output\n",
    "        decoder_activations = [a] # Start with code layer activation\n",
    "\n",
    "        for i in range(len(self.decoder_weights)):\n",
    "            z = np.dot(a, self.decoder_weights[i]) + self.decoder_biases[i]\n",
    "            # Output layer activation might differ (e.g. linear for PCA-like, or sigmoid for [0,1] data [cite: 112])\n",
    "            # Assuming sigmoid for reconstruction to match input if it was [0,1]\n",
    "            if i == len(self.decoder_weights) - 1: # Output layer\n",
    "                 if self.use_cross_entropy_loss: # implies logistic output units [cite: 112]\n",
    "                    a = sigmoid(z)\n",
    "                 else: # Assuming linear output for MSE, or if data isn't strictly [0,1]\n",
    "                    a = z # Or sigmoid(z) if input data was normalized to [0,1] and MSE is used\n",
    "            else: # Hidden layers of decoder\n",
    "                a = sigmoid(z)\n",
    "            decoder_activations.append(a)\n",
    "        return decoder_activations # Last element is the final reconstruction\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"Full forward pass through encoder and decoder.\"\"\"\n",
    "        encoder_activations = self.encode(x)\n",
    "        decoder_activations_including_code = self.decode(encoder_activations)\n",
    "        return encoder_activations, decoder_activations_including_code\n",
    "\n",
    "    def backpropagate(self, x_batch, encoder_activations, decoder_activations):\n",
    "        \"\"\"\n",
    "        Perform backpropagation and update weights.\n",
    "        \"The required gradients are easily obtained by using the chain\n",
    "         rule to backpropagate error derivatives first through the\n",
    "         decoder network and then through the encoder network\" [cite: 61]\n",
    "        \n",
    "        x_batch: Original input data for the batch\n",
    "        encoder_activations: List of activations from encoder (input, h1, h2, ..., code)\n",
    "        decoder_activations: List of activations from decoder (code, h_L-1_dec, ..., output)\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- Decoder Weight Updates ---\n",
    "        # Error at the output layer\n",
    "        # The paper implies derivative of cross-entropy for logistic units, or MSE.\n",
    "        # For cross-entropy with sigmoid output: error_delta = (reconstruction - true)\n",
    "        # For MSE with linear output: error_delta = (reconstruction - true) * derivative_of_linear (which is 1)\n",
    "        # For MSE with sigmoid output: error_delta = (reconstruction - true) * reconstruction * (1 - reconstruction)\n",
    "        \n",
    "        reconstruction = decoder_activations[-1]\n",
    "        if self.use_cross_entropy_loss: # Assumes sigmoid output\n",
    "            error_delta = (reconstruction - x_batch) # Derivative of CE with sigmoid\n",
    "        else: # Assuming MSE\n",
    "            if np.array_equal(reconstruction, sigmoid(np.dot(decoder_activations[-2], self.decoder_weights[-1]) + self.decoder_biases[-1])): # if output is sigmoid\n",
    "                 error_delta = (reconstruction - x_batch) * reconstruction * (1 - reconstruction)\n",
    "            else: # if output is linear\n",
    "                 error_delta = (reconstruction - x_batch)\n",
    "\n",
    "        # Iterate backwards through decoder layers\n",
    "        decoder_weight_grads = []\n",
    "        decoder_bias_grads = []\n",
    "\n",
    "        num_decoder_hidden_layers = len(self.decoder_weights)\n",
    "        for i in range(num_decoder_hidden_layers -1, -1, -1):\n",
    "            # current layer output a_k (decoder_activations[i+1]), prev layer output a_j (decoder_activations[i])\n",
    "            # Note: decoder_activations[0] is the code layer output.\n",
    "            # So, decoder_activations[i] is input to weights self.decoder_weights[i]\n",
    "            # and decoder_activations[i+1] is output of that layer.\n",
    "            \n",
    "            layer_input_activation = decoder_activations[i] # e.g. code for first iteration (i=0)\n",
    "            \n",
    "            # Gradient for weights\n",
    "            dw = np.dot(layer_input_activation.T, error_delta)\n",
    "            db = np.sum(error_delta, axis=0)\n",
    "            decoder_weight_grads.insert(0, dw)\n",
    "            decoder_bias_grads.insert(0, db)\n",
    "            \n",
    "            if i > 0: # Don't need to propagate error back from the first decoder layer (input to it is code)\n",
    "                # Propagate error to the previous layer (current layer's input)\n",
    "                error_delta = np.dot(error_delta, self.decoder_weights[i].T)\n",
    "                # Multiply by derivative of activation function of layer_input_activation\n",
    "                # (which is decoder_activations[i], the output of the *previous* actual layer,\n",
    "                # or the input to the *current* weights W_dec[i])\n",
    "                error_delta *= layer_input_activation * (1 - layer_input_activation) # Assuming sigmoid\n",
    "\n",
    "        # --- Encoder Weight Updates ---\n",
    "        # The error_delta now is at the output of the code layer (input to the first decoder layer)\n",
    "        # It needs to be propagated back through the encoder.\n",
    "        # The 'error_delta' from the last step of decoder backprop is delta for the code layer\n",
    "        \n",
    "        encoder_weight_grads = []\n",
    "        encoder_bias_grads = []\n",
    "\n",
    "        num_encoder_layers = len(self.encoder_weights) # e.g. 3 layers for 784-400-200-50\n",
    "        # encoder_activations: [x, a_enc1, a_enc2, code_output]\n",
    "        # So, encoder_activations[i] is input to self.encoder_weights[i]\n",
    "        # and encoder_activations[i+1] is output.\n",
    "\n",
    "        for i in range(num_encoder_layers - 1, -1, -1):\n",
    "            layer_input_activation = encoder_activations[i] # e.g. x for first iteration\n",
    "            layer_output_activation = encoder_activations[i+1] # e.g. a_enc1 for first iteration\n",
    "            \n",
    "            # Gradient for weights\n",
    "            dw = np.dot(layer_input_activation.T, error_delta)\n",
    "            db = np.sum(error_delta, axis=0)\n",
    "            encoder_weight_grads.insert(0, dw)\n",
    "            encoder_bias_grads.insert(0, db)\n",
    "            \n",
    "            if i > 0: # Don't propagate error back from input layer\n",
    "                # Propagate error to the previous layer\n",
    "                error_delta = np.dot(error_delta, self.encoder_weights[i].T)\n",
    "                # Multiply by derivative of activation function of layer_input_activation\n",
    "                # (which is encoder_activations[i], the output of the *previous* layer in encoder)\n",
    "                error_delta *= layer_input_activation * (1 - layer_input_activation) # Assuming sigmoid\n",
    "\n",
    "        # Apply updates (Gradient Descent)\n",
    "        batch_size = x_batch.shape[0]\n",
    "        for i in range(len(self.encoder_weights)):\n",
    "            self.encoder_weights[i] -= self.learning_rate * encoder_weight_grads[i] / batch_size\n",
    "            self.encoder_biases[i] -= self.learning_rate * encoder_bias_grads[i] / batch_size\n",
    "        \n",
    "        for i in range(len(self.decoder_weights)):\n",
    "            self.decoder_weights[i] -= self.learning_rate * decoder_weight_grads[i] / batch_size\n",
    "            self.decoder_biases[i] -= self.learning_rate * decoder_bias_grads[i] / batch_size\n",
    "\n",
    "\n",
    "    def fine_tune(self, data):\n",
    "        \"\"\"\n",
    "        Fine-tune the autoencoder using backpropagation. [cite: 71, 103]\n",
    "        data: Training data (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        if not self.encoder_weights: # If not pretrained\n",
    "            self._initialize_random_weights()\n",
    "\n",
    "        print(f\"Fine-tuning Deep Autoencoder: {'-'.join(map(str, self.layer_sizes))}\")\n",
    "        num_batches = int(np.ceil(data.shape[0] / self.batch_size))\n",
    "\n",
    "        for epoch in range(self.n_epochs_finetune):\n",
    "            epoch_loss = 0\n",
    "            np.random.shuffle(data) # Shuffle data each epoch\n",
    "            for i in range(num_batches):\n",
    "                batch_data = data[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                if batch_data.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                # Forward pass\n",
    "                encoder_activations, decoder_activations_full = self.forward_pass(batch_data)\n",
    "                reconstruction = decoder_activations_full[-1]\n",
    "\n",
    "                # Calculate loss\n",
    "                if self.use_cross_entropy_loss:\n",
    "                    loss = cross_entropy_loss(batch_data, reconstruction)\n",
    "                else:\n",
    "                    loss = mse_loss(batch_data, reconstruction)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Backward pass (Backpropagation)\n",
    "                self.backpropagate(batch_data, encoder_activations, decoder_activations_full)\n",
    "            \n",
    "            print(f\"  Epoch {epoch+1}/{self.n_epochs_finetune}, Fine-tuning Loss: {epoch_loss/num_batches:.6f}\")\n",
    "        print(\"Autoencoder fine-tuning complete.\\n\")\n",
    "\n",
    "    def get_code(self, data):\n",
    "        \"\"\"Get the low-dimensional code for the input data.\"\"\"\n",
    "        encoder_activations = self.encode(data)\n",
    "        return encoder_activations[-1] # The last activation in the encoder is the code\n",
    "\n",
    "    def reconstruct(self, data):\n",
    "        \"\"\"Reconstruct data using the autoencoder.\"\"\"\n",
    "        _, decoder_activations = self.forward_pass(data)\n",
    "        return decoder_activations[-1]\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # --- 1. Configuration ---\n",
    "    # Example: For MNIST-like data (28x28 images = 784 features)\n",
    "    # Autoencoder structure: 784 -> 400 -> 200 -> 50 (code layer)\n",
    "    input_dim = 784\n",
    "    # Layer sizes for the autoencoder (input -> h1 -> h2 -> code)\n",
    "    # The paper mentions structures like (28*28)-400-200-100-50-25-6 [cite: 113]\n",
    "    # or 784-1000-500-250-30 [cite: 120]\n",
    "    autoencoder_layer_sizes = [input_dim, 400, 200, 50] \n",
    "    \n",
    "    # RBM layers will be:\n",
    "    # RBM1: input_dim -> 400\n",
    "    # RBM2: 400 -> 200\n",
    "    # RBM3: 200 -> 50 (This RBM learns features that will form the code layer)\n",
    "    rbm_hidden_sizes = autoencoder_layer_sizes[1:] # [400, 200, 50]\n",
    "\n",
    "    # Create some dummy data (e.g., random binary data for simplicity)\n",
    "    # In practice, this would be your actual dataset (e.g., MNIST images)\n",
    "    # Values should be in [0,1] if using sigmoid units and cross-entropy [cite: 112]\n",
    "    print(\"Generating dummy data...\")\n",
    "    num_samples = 1000\n",
    "    train_data = np.random.rand(num_samples, input_dim).astype(np.float32)\n",
    "    # train_data = (train_data > 0.5).astype(np.float32) # For binary data\n",
    "    print(f\"Dummy data generated: {train_data.shape}\\n\")\n",
    "\n",
    "    # --- 2. Pretraining Phase ---\n",
    "    # \"Pretraining consists of learning a stack of restricted Boltzmann machines (RBMs)\" [cite: 69]\n",
    "    print(\"--- Starting RBM Pretraining Phase ---\")\n",
    "    rbm_stack = []\n",
    "    current_input_data = train_data\n",
    "    current_input_size = input_dim\n",
    "\n",
    "    for i, hidden_size in enumerate(rbm_hidden_sizes):\n",
    "        print(f\"Training RBM {i+1} ({current_input_size} -> {hidden_size})...\")\n",
    "        # RBM parameters can be tuned\n",
    "        rbm = RBM(n_visible=current_input_size, \n",
    "                  n_hidden=hidden_size, \n",
    "                  learning_rate=0.05, \n",
    "                  n_epochs=10, # Short epochs for demo\n",
    "                  batch_size=64,\n",
    "                  k=1) # CD-1\n",
    "        rbm.train(current_input_data)\n",
    "        rbm_stack.append(rbm)\n",
    "        \n",
    "        # Get the activations of the hidden layer to use as input for the next RBM [cite: 70]\n",
    "        print(f\"Transforming data with RBM {i+1} to get input for next RBM...\")\n",
    "        current_input_data = rbm.transform(current_input_data)\n",
    "        current_input_size = hidden_size\n",
    "        print(f\"New input data shape for next RBM: {current_input_data.shape}\\n\")\n",
    "    print(\"--- RBM Pretraining Complete ---\\n\")\n",
    "\n",
    "    # --- 3. Unrolling Phase & Autoencoder Initialization ---\n",
    "    # \"After the pretraining, the RBMs are \"unrolled\" to create a deep autoencoder\" [cite: 71]\n",
    "    print(\"--- Initializing Deep Autoencoder and Unrolling Weights ---\")\n",
    "    deep_ae = DeepAutoencoder(\n",
    "        layer_sizes=autoencoder_layer_sizes,\n",
    "        learning_rate=0.1, # Learning rate for fine-tuning\n",
    "        n_epochs_finetune=20, # Short epochs for demo\n",
    "        batch_size=64,\n",
    "        # The paper uses cross-entropy for images with logistic output units [cite: 112]\n",
    "        use_cross_entropy_loss=True # Set based on data and output units\n",
    "    )\n",
    "    deep_ae.initialize_weights_from_rbms(rbm_stack)\n",
    "    \n",
    "    # --- Alternative: Autoencoder without pretraining (for comparison) ---\n",
    "    # print(\"--- Initializing Deep Autoencoder RANDOMLY (no pretraining) ---\")\n",
    "    # deep_ae_random = DeepAutoencoder(\n",
    "    #     layer_sizes=autoencoder_layer_sizes,\n",
    "    #     learning_rate=0.1,\n",
    "    #     n_epochs_finetune=20,\n",
    "    #     batch_size=64,\n",
    "    #     use_cross_entropy_loss=True\n",
    "    # )\n",
    "    # # deep_ae_random._initialize_random_weights() # Called internally by fine_tune if not pretrained\n",
    "\n",
    "    # --- 4. Fine-tuning Phase ---\n",
    "    # \"...which is then fine-tuned using backpropagation of error derivatives.\" [cite: 71]\n",
    "    print(\"--- Starting Autoencoder Fine-tuning Phase (with pretrained weights) ---\")\n",
    "    deep_ae.fine_tune(train_data) # Use original full-dimensional data for fine-tuning\n",
    "\n",
    "    # print(\"--- Starting Autoencoder Fine-tuning Phase (with random weights) ---\")\n",
    "    # deep_ae_random.fine_tune(train_data)\n",
    "    # The paper notes: \"With large initial weights, autoencoders typically find poor local minima;\n",
    "    # with small initial weights, the gradients in the early layers are tiny...\n",
    "    # If the initial weights are close to a good solution, gradient descent works well\" [cite: 63, 64, 65]\n",
    "    # And \"Without pretraining, the very deep autoencoder always reconstructs the average of the\n",
    "    # training data, even after prolonged fine-tuning (8).\" (for a very deep example) [cite: 117]\n",
    "\n",
    "    # --- 5. Using the Autoencoder ---\n",
    "    print(\"--- Using the Trained Autoencoder ---\")\n",
    "    # Get low-dimensional codes\n",
    "    sample_data_for_coding = train_data[:5]\n",
    "    codes = deep_ae.get_code(sample_data_for_coding)\n",
    "    print(f\"Generated codes for 5 samples (shape: {codes.shape}):\\n{codes}\\n\")\n",
    "\n",
    "    # Reconstruct data\n",
    "    reconstructions = deep_ae.reconstruct(sample_data_for_coding)\n",
    "    print(f\"Reconstructed 5 samples (shape: {reconstructions.shape}):\\n{reconstructions}\\n\")\n",
    "    \n",
    "    original_vs_reconstructed_loss = cross_entropy_loss(sample_data_for_coding, reconstructions) if deep_ae.use_cross_entropy_loss else mse_loss(sample_data_for_coding, reconstructions)\n",
    "    print(f\"Final reconstruction loss on 5 samples: {original_vs_reconstructed_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
