{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6409730a",
   "metadata": {},
   "source": [
    "## Simple Chase example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bfe318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [ 0.          0.          0.5479121  -0.12224312 -1.2783929   0.97836334\n",
      " -0.07786063  0.6371589   0.16928375  0.51697916  0.          0.        ]\n",
      "===============================================\n",
      "Action: 2\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.          0.          0.71719587  0.39473605 -1.4476767   0.46138418\n",
      " -0.24714437  0.12017969 -0.16928375 -0.51697916]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [ 3.0000001e-01 -0.0000000e+00  5.4791212e-01 -1.2224312e-01\n",
      " -1.2783929e+00  9.7836334e-01 -7.7860631e-02  6.3715887e-01\n",
      "  1.6928375e-01  5.1697916e-01 -4.0000001e-01 -7.3046807e-14]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [-4.0000001e-01 -7.3046807e-14  7.1719587e-01  3.9473605e-01\n",
      " -1.4476767e+00  4.6138418e-01 -2.4714437e-01  1.2017969e-01\n",
      " -1.6928375e-01 -5.1697916e-01]\n",
      "===============================================\n",
      "Action: 2\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [ 2.2499999e-01 -3.0000001e-01  5.7791209e-01 -1.2224312e-01\n",
      " -1.3083929e+00  9.7836334e-01 -1.0786063e-01  6.3715887e-01\n",
      "  9.9283740e-02  5.1697916e-01  1.0000000e-01 -1.2783191e-13]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 1.0000000e-01 -1.2783191e-13  6.7719585e-01  3.9473605e-01\n",
      " -1.4076766e+00  4.6138418e-01 -2.0714438e-01  1.2017969e-01\n",
      " -9.9283740e-02 -5.1697916e-01]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [ 0.16875    -0.525       0.6004121  -0.15224312 -1.3308929   1.0083634\n",
      " -0.13036063  0.66715884  0.08678374  0.5469792  -0.23402846 -0.05277928]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [-0.23402846 -0.05277928  0.68719584  0.39473605 -1.4176766   0.46138418\n",
      " -0.21714437  0.12017969 -0.08678374 -0.5469792 ]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [ 0.1265625  -0.69375     0.6172871  -0.20474312 -1.3477678   1.0608634\n",
      " -0.14723563  0.71965885  0.04650589  0.59420127 -0.1583068   0.35088807]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [-0.1583068   0.35088807  0.66379297  0.38945812 -1.3942738   0.4666621\n",
      " -0.19374153  0.12545761 -0.04650589 -0.59420127]\n",
      "===============================================\n",
      "Action: 2\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [ 0.09492187 -0.8203125   0.6299434  -0.27411813 -1.3604242   1.1302383\n",
      " -0.15989189  0.7890339   0.01801897  0.6986651   0.4423083   0.15888537]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.4423083   0.15888537  0.64796233  0.42454693 -1.3784431   0.4315733\n",
      " -0.17791085  0.09036881 -0.01801897 -0.6986651 ]\n",
      "===============================================\n",
      "Action: 2\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.2288086  -0.6152344   0.6394355  -0.35614938 -1.3699163   1.2122695\n",
      " -0.16938408  0.87106514  0.05275761  0.79658484  1.1815629  -0.1093254 ]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 1.1815629  -0.1093254   0.69219315  0.44043547 -1.422674    0.41568476\n",
      " -0.22214168  0.07448027 -0.05275761 -0.79658484]\n",
      "===============================================\n",
      "Action: 0\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.47160643 -0.46142578  0.6165547  -0.4176728  -1.3470354   1.273793\n",
      " -0.14650321  0.9325886   0.19379476  0.8471757   1.0350736  -0.13191818]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 1.0350736  -0.13191818  0.8103494   0.42950293 -1.5408303   0.4266173\n",
      " -0.34029797  0.08541282 -0.19379476 -0.8471757 ]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.35370484 -0.64606935  0.56939405 -0.4638154  -1.2998748   1.3199356\n",
      " -0.09934257  0.97873116  0.34446278  0.8801265   0.77630526 -0.49893862]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.77630526 -0.49893862  0.9138568   0.41631112 -1.6443375   0.4398091\n",
      " -0.44380534  0.09860463 -0.34446278 -0.8801265 ]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  -0.13856798193598752\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.26527864 -0.784552    0.5340235  -0.5284223  -1.2645043   1.3845426\n",
      " -0.06397209  1.0433381   0.45746377  0.8948396   0.58222896  0.02579603]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.58222896  0.02579603  0.9914873   0.36641726 -1.721968    0.48970297\n",
      " -0.52143586  0.14849849 -0.45746377 -0.8948396 ]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  -0.9148732485742428\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.19895896 -0.888414    0.5074957  -0.6068775  -1.2379764   1.4629978\n",
      " -0.03744422  1.1217933   0.5422145   0.97587436  0.4366717  -0.380653  ]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.4366717  -0.380653    1.0497103   0.36899686 -1.780191    0.48712337\n",
      " -0.57965875  0.14591889 -0.5422145  -0.97587436]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  -1.1045305904706662\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.44921923 -0.6663105   0.4875998  -0.69571894 -1.2180805   1.5518391\n",
      " -0.01754832  1.2106347   0.6057776   1.0266504   0.32750377  0.11451027]\n",
      "===============================================\n",
      "Action: 0\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.32750377  0.11451027  1.0933774   0.33093154 -1.8238581   0.5251887\n",
      " -0.62332594  0.18398419 -0.6057776  -1.0266504 ]\n",
      "===============================================\n",
      "Action: 0\n",
      "===============================================\n",
      "reward:  -1.205331680033661\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.33691442 -0.49973288  0.44267786 -0.76234996 -1.1731586   1.6184702\n",
      "  0.0273736   1.2772657   0.6834499   1.1047325   0.24562784  0.0858827 ]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.24562784  0.0858827   1.1261277   0.34238258 -1.8566085   0.5137376\n",
      " -0.6560763   0.17253317 -0.6834499  -1.1047325 ]\n",
      "===============================================\n",
      "Action: 0\n",
      "===============================================\n",
      "reward:  -1.286924854863613\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.5526858  -0.37479967  0.40898642 -0.8123233  -1.1394672   1.6684434\n",
      "  0.06106504  1.327239    0.7417041   1.1632941   0.18422088  0.06441203]\n",
      "===============================================\n",
      "Action: 0\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.18422088  0.06441203  1.1506906   0.35097086 -1.8811713   0.50514936\n",
      " -0.6806391   0.1639449  -0.7417041  -1.1632941 ]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  -1.3517243946565933\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.41451436 -0.28109974  0.35371783 -0.8498032  -1.0841986   1.7059234\n",
      "  0.11633362  1.3647189   0.8153948   1.2072153   0.13816565  0.448309  ]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.13816565  0.448309    1.1691127   0.35741207 -1.8995935   0.4987082\n",
      " -0.69906116  0.1575037  -0.8153948  -1.2072153 ]\n",
      "===============================================\n",
      "Action: 2\n",
      "===============================================\n",
      "reward:  -1.4024564166721496\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.6108858  -0.2108248   0.3122664  -0.8779132  -1.0427471   1.7340335\n",
      "  0.15778506  1.392829    0.8706628   1.2801561   0.50362426  0.33623177]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.50362426  0.33623177  1.1829292   0.40224296 -1.91341     0.45387727\n",
      " -0.71287775  0.11267279 -0.8706628  -1.2801561 ]\n",
      "===============================================\n",
      "Action: 3\n",
      "===============================================\n",
      "reward:  -1.441751096837002\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.45816433 -0.45811862  0.25117782 -0.8989957  -0.9816586   1.7551159\n",
      "  0.21887363  1.4139115   0.9821138   1.3348618   0.37771818 -0.14782618]\n",
      "===============================================\n",
      "Action: 2\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.37771818 -0.14782618  1.2332916   0.43586615 -1.9637724   0.4202541\n",
      " -0.76324016  0.07904962 -0.9821138  -1.3348618 ]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  -1.5945367626850802\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.04362325 -0.34358895  0.2053614  -0.9448075  -0.93584216  1.8009278\n",
      "  0.26469007  1.4597232   1.0657021   1.3658911   0.28328863  0.28913036]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.28328863  0.28913036  1.2710634   0.4210835  -2.0015442   0.43503672\n",
      " -0.801012    0.09383223 -1.0657021  -1.3658911 ]\n",
      "===============================================\n",
      "Action: 0\n",
      "===============================================\n",
      "reward:  -1.7196605209009281\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.03271743  0.04230829  0.20099907 -0.97916645 -0.9314799   1.8352866\n",
      "  0.2690524   1.4940822   1.0983932   1.429163    0.21246648  0.21684778]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.21246648  0.21684778  1.2993923   0.44999656 -2.0298731   0.40612367\n",
      " -0.8293409   0.0649192  -1.0983932  -1.429163  ]\n",
      "===============================================\n",
      "Action: 2\n",
      "===============================================\n",
      "reward:  -1.8199055920737004\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.02453808  0.3317312   0.19772732 -0.9749356  -0.9282081   1.8310559\n",
      "  0.27232414  1.4898514   1.1229117   1.4466169   0.55934983  0.16263583]\n",
      "===============================================\n",
      "Action: 2\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.55934983  0.16263583  1.320639    0.47168133 -2.0511198   0.3844389\n",
      " -0.8505875   0.04323442 -1.1229117  -1.4466169 ]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  -1.8989059848056835\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [ 0.28159645  0.24879842  0.19527352 -0.9417625  -0.9257543   1.7978827\n",
      "  0.27477795  1.4566783   1.1813004   1.4297074   0.4195124   0.5219769 ]\n",
      "===============================================\n",
      "Action: 0\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.4195124   0.5219769   1.3765739   0.4879449  -2.1070547   0.36817533\n",
      " -0.9065225   0.02697084 -1.1813004  -1.4297074 ]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  -2.1236746094246697\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [ 0.21119733  0.18659881  0.22343317 -0.91688263 -0.9539139   1.7730029\n",
      "  0.2466183   1.4317983   1.1950921   1.4570253   0.3146343   0.7914826 ]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.3146343   0.7914826   1.4185252   0.5401426  -2.149006    0.31597763\n",
      " -0.94847375 -0.02522685 -1.1950921  -1.4570253 ]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  -2.309544625668146\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.141602    0.1399491   0.2445529  -0.89822274 -0.9750337   1.754343\n",
      "  0.22549857  1.4131385   1.2054358   1.5175136   0.23597573  0.993612  ]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.23597573  0.993612    1.4499886   0.6192909  -2.1804693   0.23682937\n",
      " -0.97993714 -0.10437512 -1.2054358  -1.5175136 ]\n",
      "===============================================\n",
      "Action: 4\n",
      "===============================================\n",
      "reward:  -2.459547112635284\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.1062015   0.40496182  0.2303927  -0.8842279  -0.9608735   1.7403481\n",
      "  0.23965877  1.3991436   1.2431935   1.6028799   0.17698179  1.145209  ]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [ 0.17698179  1.145209    1.4735862   0.71865207 -2.204067    0.13746816\n",
      " -1.0035347  -0.20373632 -1.2431935  -1.6028799 ]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  -2.5784085738028875\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.37965113  0.30372137  0.21977255 -0.84373164 -0.9502533   1.6998519\n",
      "  0.25027892  1.3586475   1.2715118   1.6769047  -0.26726365  0.85890675]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [-0.26726365  0.85890675  1.4912844   0.833173   -2.221765    0.02294727\n",
      " -1.021233   -0.3182572  -1.2715118  -1.6769047 ]\n",
      "===============================================\n",
      "Action: 1\n",
      "===============================================\n",
      "reward:  -2.6713093220145594\n",
      "===============================================\n",
      "Agent:  adversary_0\n",
      "===============================================\n",
      "Observation:  [-0.5847384   0.22779103  0.18180743 -0.81335956 -0.9122882   1.6694797\n",
      "  0.28824404  1.3282753   1.2827506   1.7324232  -0.6004477   0.64418006]\n",
      "===============================================\n",
      "Action: None\n",
      "===============================================\n",
      "reward:  0.0\n",
      "===============================================\n",
      "Agent:  agent_0\n",
      "===============================================\n",
      "Observation:  [-0.6004477   0.64418006  1.464558    0.9190636  -2.1950388  -0.06294341\n",
      " -0.99450654 -0.4041479  -1.2827506  -1.7324232 ]\n",
      "===============================================\n",
      "Action: None\n",
      "===============================================\n",
      "reward:  -2.722906073412529\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# https://pettingzoo.farama.org/environments/mpe/simple_push/\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "\n",
    "env = simple_tag_v3.env(num_good=1, num_adversaries=1 , render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "verbose = False\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    if verbose == True:\n",
    "\n",
    "        print(\"Agent: \", agent)\n",
    "        print(\"===============================================\")\n",
    "        print(\"Observation: \", observation)\n",
    "        print(\"===============================================\")\n",
    "        print(\"Action:\", action)\n",
    "        print(\"===============================================\")\n",
    "        print(\"reward: \", reward)\n",
    "        print(\"===============================================\")\n",
    "\n",
    "    \n",
    "\n",
    "    env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ce5149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adversary_0', 'agent_0']\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.34683114,  0.80838966,  0.8542746 ], dtype=float32), 'agent_0': array([-0.38499805, -1.2011057 , -0.38499805, -1.2011057 , -0.80838966,\n",
      "       -0.8542746 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.5473136829039504, 'agent_0': -0.7139866281272493})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.29683113,  0.80838966,  0.9042746 ], dtype=float32), 'agent_0': array([-0.38499805, -1.2011057 , -0.38499805, -1.2011057 , -0.80838966,\n",
      "       -0.9042746 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.5170775131871352, 'agent_0': -0.7442227978440645})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.20933113,  0.75838965,  0.9917746 ], dtype=float32), 'agent_0': array([-0.33499807, -1.2011057 , -0.33499807, -1.2011057 , -0.75838965,\n",
      "       -0.9917746 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(4), 'agent_0': np.int64(3)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.4723134098838689, 'agent_0': -0.7746343307878638})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.19370613,  0.7208896 ,  0.9573996 ], dtype=float32), 'agent_0': array([-0.29749808, -1.1511058 , -0.29749808, -1.1511058 , -0.7208896 ,\n",
      "       -0.9573996 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(4), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.46559907647618154, 'agent_0': -0.723328797892397})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.23198737,  0.74276465,  0.8816183 ], dtype=float32), 'agent_0': array([-0.31937307, -1.1136057 , -0.31937307, -1.1136057 , -0.74276465,\n",
      "       -0.8816183 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(4), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.48278212433807954, 'agent_0': -0.6757155524312072})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.31069833,  0.7591709 ,  0.7747824 ], dtype=float32), 'agent_0': array([-0.3357793, -1.0854807, -0.3357793, -1.0854807, -0.7591709,\n",
      "       -0.7747824], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(2), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.525160812858757, 'agent_0': -0.6110681052887186})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.3733916 , -0.36973152,  0.7214756 ,  0.6946555 ], dtype=float32), 'agent_0': array([-0.348084 , -1.064387 , -0.348084 , -1.064387 , -0.7214756,\n",
      "       -0.6946555], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(1), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.5254737608568437, 'agent_0': -0.5943843136652033})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.3858916, -0.4140064,  0.6932041,  0.6345602], dtype=float32), 'agent_0': array([-0.30731252, -1.0485667 , -0.30731252, -1.0485667 , -0.6932041 ,\n",
      "       -0.6345602 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(4), 'agent_0': np.int64(3)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.5659625727931983, 'agent_0': -0.5267098534639973})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.3952666 , -0.4972126 ,  0.67200047,  0.48948884], dtype=float32), 'agent_0': array([-0.2767339 , -0.9867014 , -0.2767339 , -0.9867014 , -0.67200047,\n",
      "       -0.48948884], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(0), 'agent_0': np.int64(3)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.6351818971749154, 'agent_0': -0.3895919138802525})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.40229782, -0.5596172 ,  0.65609777,  0.33068526], dtype=float32), 'agent_0': array([-0.25379995, -0.8903025 , -0.25379995, -0.8903025 , -0.65609777,\n",
      "       -0.33068526], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(0), 'agent_0': np.int64(4)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.6892133156868342, 'agent_0': -0.23655823228842865})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.4075713 , -0.6064207 ,  0.64417076,  0.2615826 ], dtype=float32), 'agent_0': array([-0.23659948, -0.8680033 , -0.23659948, -0.8680033 , -0.64417076,\n",
      "       -0.2615826 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(1), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.7306575226324428, 'agent_0': -0.16901411497514995})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.46152636, -0.6415233 ,  0.7352255 ,  0.2097556 ], dtype=float32), 'agent_0': array([-0.27369913, -0.8512789 , -0.27369913, -0.8512789 , -0.7352255 ,\n",
      "       -0.2097556 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(4), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.7902902837682868, 'agent_0': -0.10390600197663691})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.50199264, -0.71785027,  0.7535165 ,  0.12088535], dtype=float32), 'agent_0': array([-0.25152385, -0.83873564, -0.25152385, -0.83873564, -0.7535165 ,\n",
      "       -0.12088535], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.8759598365730255, 'agent_0': 0.000321963312686524})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.5323424 , -0.7250955 ,  0.7172348 ,  0.10423266], dtype=float32), 'agent_0': array([-0.18489242, -0.8293281 , -0.18489242, -0.8293281 , -0.7172348 ,\n",
      "       -0.10423266], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.8995286986372396, 'agent_0': 0.04984030116570093})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.5551047 , -0.6805294 ,  0.69002354,  0.14174314], dtype=float32), 'agent_0': array([-0.13491882, -0.82227254, -0.13491882, -0.82227254, -0.69002354,\n",
      "       -0.14174314], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(2), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.8782149354261759, 'agent_0': 0.04494714096765717})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.52217644, -0.6471048 ,  0.6196151 ,  0.16987601], dtype=float32), 'agent_0': array([-0.09743863, -0.81698084, -0.09743863, -0.81698084, -0.6196151 ,\n",
      "       -0.16987601], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.8315123964031518, 'agent_0': 0.008741484764053342})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.4974802 , -0.5720364 ,  0.6168087 ,  0.24097565], dtype=float32), 'agent_0': array([-0.11932849, -0.81301206, -0.11932849, -0.81301206, -0.6168087 ,\n",
      "       -0.24097565], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.7580977528401887, 'agent_0': -0.06362476450814003})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.47895804, -0.46573508,  0.61470395,  0.3443004 ], dtype=float32), 'agent_0': array([-0.13574588, -0.81003547, -0.13574588, -0.81003547, -0.61470395,\n",
      "       -0.3443004 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(1), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.66806435659277, 'agent_0': -0.15326652437241395})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.51506644, -0.3860091 ,  0.6631254 ,  0.42179394], dtype=float32), 'agent_0': array([-0.14805894, -0.80780303, -0.14805894, -0.80780303, -0.6631254 ,\n",
      "       -0.42179394], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(0), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.6436586480010066, 'agent_0': -0.17760086908283235})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.5421477, -0.3262146,  0.6494414,  0.4799141], dtype=float32), 'agent_0': array([-0.10729372, -0.8061287 , -0.10729372, -0.8061287 , -0.6494414 ,\n",
      "       -0.4799141 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(2), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.6327243649147697, 'agent_0': -0.18051326174064608})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.5124587 , -0.28136873,  0.6391785 ,  0.52350426], dtype=float32), 'agent_0': array([-0.1267198 , -0.804873  , -0.1267198 , -0.804873  , -0.6391785 ,\n",
      "       -0.52350426], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(2), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.5846214787375595, 'agent_0': -0.23016585280384727})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.44019192, -0.24773434,  0.6314813 ,  0.5561968 ], dtype=float32), 'agent_0': array([-0.19128937, -0.8039312 , -0.19128937, -0.8039312 , -0.6314813 ,\n",
      "       -0.5561968 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(0), 'agent_0': np.int64(3)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.505115057073559, 'agent_0': -0.3212607187043024})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.38599184, -0.22250853,  0.6257084 ,  0.53071624], dtype=float32), 'agent_0': array([-0.23971654, -0.7532248 , -0.23971654, -0.7532248 , -0.6257084 ,\n",
      "       -0.53071624], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(2), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.4455330978485435, 'agent_0': -0.34491716340929096})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.29534176, -0.20358919,  0.5213787 ,  0.51160586], dtype=float32), 'agent_0': array([-0.22603692, -0.715195  , -0.22603692, -0.715195  , -0.5213787 ,\n",
      "       -0.51160586], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.358713417993204, 'agent_0': -0.39135099282457864})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.22735423, -0.13939966,  0.49313143,  0.54727304], dtype=float32), 'agent_0': array([-0.2657772 , -0.6866727 , -0.2657772 , -0.6866727 , -0.49313143,\n",
      "       -0.54727304], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(0), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.2666874780842228, 'agent_0': -0.4696255845523323})\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "\n",
    "env = simple_adversary_v3.parallel_env(render_mode=\"human\", N=1)\n",
    "observations, infos = env.reset()\n",
    "verbose = True\n",
    "print(env.agents)\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    \n",
    "    if verbose == True:\n",
    "\n",
    "        # print(\"Agent: \", agent)\n",
    "        # print(\"===============================================\")\n",
    "        print(\"Observation: \", observations)\n",
    "        print(\"===============================================\")\n",
    "        print(\"Action:\", actions)\n",
    "        print(\"===============================================\")\n",
    "        print(\"reward: \", rewards)\n",
    "        print(\"===============================================\")\n",
    "\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adversary_0', 'agent_0']\n"
     ]
    }
   ],
   "source": [
    "env = simple_adversary_v3.parallel_env(render_mode=\"human\", N=1)\n",
    "env.reset()\n",
    "print(env.agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af125971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = simple_tag_v3.env(num_good=1, num_adversaries=1, render_mode=\"rgb_array\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "\n",
    "# --- Independent Q-Learning (IQL) example skeleton ---\n",
    "alpha = 0.01\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay_steps = 80000\n",
    "decay_rate = (epsilon - epsilon_min) / epsilon_decay_steps\n",
    "\n",
    "Q_tables = {agent: defaultdict(lambda: np.zeros(env.action_space(agent).n))\n",
    "            for agent in env.agents}\n",
    "\n",
    "def get_state(obs):\n",
    "    return tuple(np.round(obs, 1))  # discretize continuous obs\n",
    "\n",
    "returns = []\n",
    "for t in tqdm(range(10)):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    done = {a: False for a in env.agents}\n",
    "\n",
    "    while not all(done.values()):\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, term, trunc, info = env.last()\n",
    "            done[agent] = term or trunc\n",
    "            if done[agent]:\n",
    "                env.step(None)\n",
    "                continue\n",
    "            state = get_state(obs)\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space(agent).sample()\n",
    "            else:\n",
    "                action = np.argmax(Q_tables[agent][state])\n",
    "\n",
    "            env.step(action)\n",
    "            next_obs, reward, term, trunc, _ = env.last()\n",
    "            next_state = get_state(next_obs)\n",
    "            done[agent] = term or trunc\n",
    "\n",
    "            best_next = np.max(Q_tables[agent][next_state])\n",
    "            Q_tables[agent][state][action] += alpha * (reward + gamma * best_next - Q_tables[agent][state][action])\n",
    "            total_reward += reward\n",
    "\n",
    "    returns.append(total_reward)\n",
    "\n",
    "    # Decay epsilon\n",
    "    if t < epsilon_decay_steps:\n",
    "        epsilon -= decay_rate\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "# Plot smoothed curve\n",
    "window = 1000\n",
    "smoothed = np.convolve(returns, np.ones(window)/window, mode='valid')\n",
    "plt.plot(smoothed)\n",
    "plt.xlabel('Environment time steps')\n",
    "plt.ylabel('Evaluation returns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0b5bd",
   "metadata": {},
   "source": [
    "## Joint Action Learning with Agent Modeling using Deep Q-Networks (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58db26b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import collections\n",
    "\n",
    "env = simple_tag_v3.parallel_env(num_good=1, num_adversaries=1, render_mode=None)\n",
    "env.reset(seed=42)\n",
    "\n",
    "alpha = 0.01\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay_steps = 80000\n",
    "decay_rate = (epsilon - epsilon_min) / epsilon_decay_steps\n",
    "episodes = 1000\n",
    "BUFFER_CAPACITY = 1000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99       # Discount factor\n",
    "TAU = 0.005        # For soft target network updates\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fde6e736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        , -0.74377275, -0.09922812,  1.0027299 ,\n",
       "        0.680199  ,  0.6419183 , -0.39174217,  0.4853688 ,  0.9527581 ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.25840396,\n",
       "        0.85353   ,  0.51736116, -0.27255908,  0.15654951, -1.3445003 ,\n",
       "       -0.4853688 , -0.9527581 ], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env.observation_space('adversary_0').shape[0]\n",
    "obs, info = env.reset()\n",
    "np.concat((obs['adversary_0'], obs['agent_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26c220c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deep Q-Network (DQN)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ModelNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.net(x), dim=-1)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[idx] for idx in batch])\n",
    "        return (np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- Joint Action Learning with Agent Modeling using Deep Q-Networks (DQN) ---\n",
    "\n",
    "global_state_dim = env.observation_space('adversary_0').shape[0] + env.observation_space('agent_0').shape[0]\n",
    "action_dim_0 = env.action_space('agent_0').n\n",
    "action_dim_1 = env.action_space('adversary_0').n\n",
    "joint_action_dim = action_dim_0 * action_dim_1\n",
    "\n",
    "\n",
    "\n",
    "# --- Agent 0 (Agent_0) ---\n",
    "# Q-Net learns Q_0(s, a_0, a_1)\n",
    "# agent_net = QNetwork(global_state_dim, joint_action_dim)\n",
    "# agent_target_net = QNetwork(global_state_dim, joint_action_dim)\n",
    "# agent_target_net.load_state_dict(agent_net.state_dict())\n",
    "\n",
    "# Model-Net learns pi_1(a_1 | s)\n",
    "# It models Agent 1 (the opponent)\n",
    "# agent_model = ModelNetwork(global_state_dim, action_dim_1) \n",
    "\n",
    "# --- Agent 0 (Agent_0) ---\n",
    "agent_net = QNetwork(global_state_dim, joint_action_dim).to(device)\n",
    "agent_target_net = QNetwork(global_state_dim, joint_action_dim).to(device)\n",
    "agent_target_net.load_state_dict(agent_net.state_dict())\n",
    "agent_model = ModelNetwork(global_state_dim, action_dim_1).to(device) \n",
    "\n",
    "\n",
    "\n",
    "# --- Agent 1 (Adversary_0) ---\n",
    "# Q-Net learns Q_1(s, a_0, a_1)\n",
    "# adversary_net = QNetwork(global_state_dim, joint_action_dim)\n",
    "# adversary_target_net = QNetwork(global_state_dim, joint_action_dim)\n",
    "# adversary_target_net.load_state_dict(adversary_net.state_dict())\n",
    "\n",
    "# Model-Net learns pi_0(a_0 | s)\n",
    "# It models Agent 0 (the opponent)\n",
    "# adversary_model = ModelNetwork(global_state_dim, action_dim_0)\n",
    "\n",
    "adversary_net = QNetwork(global_state_dim, joint_action_dim).to(device)\n",
    "adversary_target_net = QNetwork(global_state_dim, joint_action_dim).to(device)\n",
    "adversary_target_net.load_state_dict(adversary_net.state_dict())\n",
    "adversary_model = ModelNetwork(global_state_dim, action_dim_0).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "agent_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "adversary_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "\n",
    "q_optimizer_0 = torch.optim.Adam(agent_net.parameters(), lr=alpha)\n",
    "q_optimizer_1 = torch.optim.Adam(adversary_net.parameters(), lr=alpha)\n",
    "\n",
    "model_optimizer_0 = torch.optim.Adam(agent_model.parameters(), lr=alpha)\n",
    "model_optimizer_1 = torch.optim.Adam(adversary_model.parameters(), lr=alpha)\n",
    "\n",
    "model_loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d41cd851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:31<00:00,  3.01it/s, Avg_M_Loss=1.6480, Avg_Q_Loss=3.4614, Avg_Rwd_A0=-12.86, Avg_Rwd_A1=0.70, Epsilon=0.988]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "recent_rewards_agent_0 = collections.deque(maxlen=100)\n",
    "recent_rewards_agent_1 = collections.deque(maxlen=100)\n",
    "progress_bar = tqdm(range(episodes))\n",
    "for episode in progress_bar:\n",
    "    episode_reward_agent_0 = 0\n",
    "    episode_reward_agent_1 = 0\n",
    "    cumulative_q_loss_0 = 0\n",
    "    cumulative_q_loss_1 = 0\n",
    "    cumulative_model_loss_0 = 0\n",
    "    cumulative_model_loss_1 = 0\n",
    "    \n",
    "    episode_steps = 0\n",
    "    num_train_steps = 0 # To average loss correctly\n",
    "\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "    while not done:\n",
    "        global_state = np.concatenate((obs['adversary_0'], obs['agent_0']))\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(global_state).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            # Get Q_0(s, a_0, a_1) for ALL joint actions\n",
    "            # Output shape: [1, joint_action_dim]\n",
    "            all_joint_q_agent = agent_net(state_tensor) \n",
    "\n",
    "            # Get pi_1(a_1 | s) -> Agent 0's model of Agent 1\n",
    "            # Output shape: [1, action_dim_1]\n",
    "            probs_opponent_1 = agent_model(state_tensor) \n",
    "\n",
    "            # Reshape Q-values into a matrix: [action_dim_0, action_dim_1]\n",
    "            # Each row 'a0' contains Q-values for all opponent actions 'a1'\n",
    "            q_matrix_agent = all_joint_q_agent.view(action_dim_0, action_dim_1)\n",
    "\n",
    "            # Reshape opponent probs into a column vector: [action_dim_1, 1]\n",
    "            probs_opponent_1_vec = probs_opponent_1.view(action_dim_1, 1)\n",
    "\n",
    "            # This is Equation 6.17: AV_0 = Q_0 * pi_1\n",
    "            # (dim_0, dim_1) @ (dim_1, 1) -> (dim_0, 1)\n",
    "            action_values_agent_tensor = torch.matmul(q_matrix_agent, probs_opponent_1_vec)\n",
    "            \n",
    "            # Squeeze to get a 1D vector of action values for Agent 0\n",
    "            # Shape: [action_dim_0]\n",
    "            action_values_agent = action_values_agent_tensor.squeeze()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get Q_1(s, a_0, a_1) for ALL joint actions\n",
    "            # Output shape: [1, joint_action_dim]\n",
    "            all_joint_q_adversary = adversary_net(state_tensor)\n",
    "            \n",
    "            # Get pi_0(a_0 | s) -> Agent 1's model of Agent 0\n",
    "            # Output shape: [1, action_dim_0]\n",
    "            probs_opponent_0 = adversary_model(state_tensor)\n",
    "\n",
    "            # Reshape Q-values into a matrix: [action_dim_0, action_dim_1]\n",
    "            q_matrix_adversary = all_joint_q_adversary.view(action_dim_0, action_dim_1)\n",
    "\n",
    "            # Reshape *this* opponent's probs (Agent 0) into a column vector: [action_dim_0, 1]\n",
    "            probs_opponent_0_vec = probs_opponent_0.view(action_dim_0, 1)\n",
    "\n",
    "            # This is Equation 6.17 for Agent 1: AV_1 = (Q_1^T) * pi_0\n",
    "            # The sum is over a_0, so we must transpose the Q-matrix first.\n",
    "            # (dim_1, dim_0) @ (dim_0, 1) -> (dim_1, 1)\n",
    "            action_values_adversary_tensor = torch.matmul(q_matrix_adversary.T, probs_opponent_0_vec)\n",
    "            \n",
    "            # Squeeze to get a 1D vector of action values for Agent 1\n",
    "            # Shape: [action_dim_1]\n",
    "            action_values_adversary = action_values_adversary_tensor.squeeze()\n",
    "\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            agent_action = np.random.choice(action_dim_0)\n",
    "            adversary_action = np.random.choice(action_dim_1)\n",
    "        else:\n",
    "            agent_action = torch.argmax(action_values_agent).item()\n",
    "            adversary_action = torch.argmax(action_values_adversary).item()\n",
    "\n",
    "        actions = {'adversary_0': adversary_action, \"agent_0\": agent_action}\n",
    "        next_obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        done = all(terminations.values()) or all(truncations.values())\n",
    "        global_next_obs = np.concatenate((next_obs['adversary_0'], next_obs['agent_0']))\n",
    "        episode_reward_agent_0 += rewards['agent_0']\n",
    "        episode_reward_agent_1 += rewards['adversary_0']\n",
    "        episode_steps += 1\n",
    "\n",
    "        # === 1. TRAIN AGENT MODELS (Line 11) ===\n",
    "       \n",
    "        # We use NLLLoss (Negative Log Likelihood Loss) because your\n",
    "        # ModelNetwork outputs probabilities (after softmax).\n",
    "\n",
    "\n",
    "        # --- Train agent_model (Agent 0's model of Agent 1) ---\n",
    "        model_optimizer_0.zero_grad()\n",
    "        probs_opponent_1 = agent_model(state_tensor)\n",
    "        log_probs_1 = torch.log(probs_opponent_1 + 1e-9)\n",
    "        target_action_1 = torch.LongTensor([adversary_action]).to(device)\n",
    "\n",
    "        model_loss_0 = model_loss_fn(log_probs_1, target_action_1)\n",
    "        cumulative_model_loss_0 += model_loss_0.item()\n",
    "        model_loss_0.backward()\n",
    "        model_optimizer_0.step()\n",
    "\n",
    "\n",
    "        model_optimizer_1.zero_grad()\n",
    "        probs_opponent_0 = adversary_model(state_tensor)\n",
    "        log_probs_0 = torch.log(probs_opponent_0 + 1e-9)\n",
    "        target_action_0 = torch.LongTensor([agent_action]).to(device)\n",
    "\n",
    "        model_loss_1 = model_loss_fn(log_probs_0, target_action_0)\n",
    "        cumulative_model_loss_1 += model_loss_1.item()\n",
    "        model_loss_1.backward()\n",
    "        model_optimizer_1.step()\n",
    "\n",
    "        # =================================================================\n",
    "        # === 2. UPDATE BUFFERS (Line 9) ===\n",
    "        # We must convert the two separate actions into one joint_action index\n",
    "        # to store in the buffer, as this is what our Q-network learns.\n",
    "        # Formula: (a0 * num_actions_1) + a1\n",
    "        joint_action = agent_action * action_dim_1 + adversary_action\n",
    "        \n",
    "        agent_buffer.push(global_state, joint_action, rewards['agent_0'], global_next_obs, done)\n",
    "        \n",
    "        adversary_buffer.push(global_state, joint_action, rewards['adversary_0'], global_next_obs, done)\n",
    "\n",
    "        # =================================================================\n",
    "        # === 3. TRAIN Q-NETWORKS (Line 12) ===\n",
    "        # This is the Bellman update, using the AV(s') as the target.\n",
    "        \n",
    "        q_loss_fn = nn.MSELoss()\n",
    "\n",
    "        if len(agent_buffer) >= BATCH_SIZE:\n",
    "            states_b, joint_actions_b, rewards_b, next_states_b, dones_b = agent_buffer.sample(BATCH_SIZE)\n",
    "            \n",
    "            states_b = torch.FloatTensor(states_b).to(device)\n",
    "            joint_actions_b = torch.LongTensor(joint_actions_b).unsqueeze(1).to(device) # Shape: [B, 1]\n",
    "            rewards_b = torch.FloatTensor(rewards_b).unsqueeze(1).to(device)      # Shape: [B, 1]\n",
    "            next_states_b = torch.FloatTensor(next_states_b).to(device)\n",
    "            dones_b = torch.FloatTensor(dones_b).unsqueeze(1).to(device)          # Shape: [B, 1]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q_next_target_0 = agent_target_net(next_states_b) # [B, joint_action_dim]\n",
    "                # Get pi_1(a_1 | s') from the *opponent model*\n",
    "                model_next_1 = agent_model(next_states_b) # [B, action_dim_1]\n",
    "                \n",
    "                # Reshape for batch matrix multiplication\n",
    "                q_matrix_next_0 = q_next_target_0.view(BATCH_SIZE, action_dim_0, action_dim_1)\n",
    "                model_vec_next_1 = model_next_1.unsqueeze(2) # Shape: [B, action_dim_1, 1]\n",
    "                \n",
    "                # AV_0(s', a_0) = Q_target_0 * pi_1\n",
    "                av_next_0 = torch.bmm(q_matrix_next_0, model_vec_next_1) # Shape: [B, action_dim_0, 1]\n",
    "                \n",
    "                # max_a0 AV_0(s', a_0)\n",
    "                max_av_next_0, _ = torch.max(av_next_0, dim=1) # Shape: [B, 1]\n",
    "                \n",
    "                # Bellman Target: y_0 = r + gamma * max_AV * (1 - done)\n",
    "                target_q_0 = rewards_b + (GAMMA * max_av_next_0 * (1 - dones_b))\n",
    "\n",
    "            # --- Get Current Q-value: Q_0(s, a) ---\n",
    "            q_current_0 = agent_net(states_b) # [B, joint_action_dim]\n",
    "            # Get Q_0 for the *specific* joint action taken from the buffer\n",
    "            q_current_0_selected = q_current_0.gather(1, joint_actions_b) # [B, 1]\n",
    "            \n",
    "            # --- Calculate Loss and Optimize ---\n",
    "            q_loss_0 = q_loss_fn(q_current_0_selected, target_q_0)\n",
    "            q_optimizer_0.zero_grad()\n",
    "            cumulative_q_loss_0 += q_loss_0.item()\n",
    "            num_train_steps += 1 # Increment this *only* when you train\n",
    "            q_loss_0.backward()\n",
    "            q_optimizer_0.step()\n",
    "\n",
    "            # --- Soft update target network ---\n",
    "            for target_param, local_param in zip(agent_target_net.parameters(), agent_net.parameters()):\n",
    "                target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)\n",
    "            \n",
    "        # --- Train Agent 1's Q-Network (Adversary) ---\n",
    "        if len(adversary_buffer) >= BATCH_SIZE:\n",
    "            # Sample a batch\n",
    "            states_b, joint_actions_b, rewards_b, next_states_b, dones_b = adversary_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            # Convert to Tensors\n",
    "            states_b = torch.FloatTensor(states_b).to(device)\n",
    "            joint_actions_b = torch.LongTensor(joint_actions_b).unsqueeze(1).to(device)\n",
    "            rewards_b = torch.FloatTensor(rewards_b).unsqueeze(1).to(device)\n",
    "            next_states_b = torch.FloatTensor(next_states_b).to(device)\n",
    "            dones_b = torch.FloatTensor(dones_b).unsqueeze(1).to(device)\n",
    "\n",
    "            # --- Calculate Target y_1 ---\n",
    "            # y_1 = r + gamma * max_a1' AV_1(s', a_1')\n",
    "            with torch.no_grad():\n",
    "                # Get Q_target_1(s', a_0, a_1) from the *target network*\n",
    "                q_next_target_1 = adversary_target_net(next_states_b) # [B, joint_action_dim]\n",
    "                # Get pi_0(a_0 | s') from the *opponent model*\n",
    "                model_next_0 = adversary_model(next_states_b) # [B, action_dim_0]\n",
    "                \n",
    "                # Reshape for batch matrix multiplication\n",
    "                q_matrix_next_1 = q_next_target_1.view(BATCH_SIZE, action_dim_0, action_dim_1)\n",
    "                model_vec_next_0 = model_next_0.unsqueeze(2) # [B, action_dim_0, 1]\n",
    "                \n",
    "                # AV_1(s', a_1) = (Q_target_1^T) * pi_0\n",
    "                # We transpose the Q-matrix to sum over a_0\n",
    "                av_next_1 = torch.bmm(q_matrix_next_1.transpose(1, 2), model_vec_next_0) # [B, action_dim_1, 1]\n",
    "                \n",
    "                # max_a1 AV_1(s', a_1)\n",
    "                max_av_next_1, _ = torch.max(av_next_1, dim=1) # [B, 1]\n",
    "                \n",
    "                # Bellman Target: y_1 = r + gamma * max_AV * (1 - done)\n",
    "                target_q_1 = rewards_b + (GAMMA * max_av_next_1 * (1 - dones_b))\n",
    "                \n",
    "            # --- Get Current Q-value: Q_1(s, a) ---\n",
    "            q_current_1 = adversary_net(states_b)\n",
    "            q_current_1_selected = q_current_1.gather(1, joint_actions_b)\n",
    "            \n",
    "            # --- Calculate Loss and Optimize ---\n",
    "            q_loss_1 = q_loss_fn(q_current_1_selected, target_q_1)\n",
    "            q_optimizer_1.zero_grad()\n",
    "            cumulative_q_loss_1 += q_loss_1.item()\n",
    "            q_loss_1.backward()\n",
    "            q_optimizer_1.step()\n",
    "\n",
    "            # --- Soft update target network ---\n",
    "            for target_param, local_param in zip(adversary_target_net.parameters(), adversary_net.parameters()):\n",
    "                target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)\n",
    "\n",
    "        # =================================================================\n",
    "        # === 4. PREPARE FOR NEXT STEP ===\n",
    "        obs = next_obs\n",
    "        global_state = global_next_obs\n",
    "    \n",
    "    # --- (End of `while not done` loop) ---\n",
    "    avg_q_loss_0 = cumulative_q_loss_0 / num_train_steps if num_train_steps > 0 else 0\n",
    "    avg_q_loss_1 = cumulative_q_loss_1 / num_train_steps if num_train_steps > 0 else 0\n",
    "    \n",
    "    avg_model_loss_0 = cumulative_model_loss_0 / episode_steps if episode_steps > 0 else 0\n",
    "    avg_model_loss_1 = cumulative_model_loss_1 / episode_steps if episode_steps > 0 else 0\n",
    "\n",
    "    recent_rewards_agent_0.append(episode_reward_agent_0)\n",
    "    recent_rewards_agent_1.append(episode_reward_agent_1)\n",
    "    \n",
    "    avg_reward_0 = sum(recent_rewards_agent_0) / len(recent_rewards_agent_0)\n",
    "    avg_reward_1 = sum(recent_rewards_agent_1) / len(recent_rewards_agent_1)\n",
    "    \n",
    "    progress_bar.set_postfix(\n",
    "        Avg_Rwd_A0=f\"{avg_reward_0:.2f}\",\n",
    "        Avg_Rwd_A1=f\"{avg_reward_1:.2f}\",\n",
    "        Epsilon=f\"{epsilon:.3f}\",\n",
    "        Avg_Q_Loss=f\"{avg_q_loss_0:.4f}\", # Added a bit more precision\n",
    "        Avg_M_Loss=f\"{avg_model_loss_0:.4f}\"\n",
    "    )\n",
    "    # tqdm.write(desc) # Use tqdm.write instead of print to avoid breaking the bar\n",
    "\n",
    "    # Decay epsilon (your existing code)\n",
    "    if episode < epsilon_decay_steps:\n",
    "        epsilon -= decay_rate\n",
    "        epsilon = max(epsilon, epsilon_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661eb978",
   "metadata": {},
   "source": [
    "## Soccer Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . .\n",
      ". . . B* .\n",
      "A . . . .\n",
      ". . . . .\n",
      "Ball: B\n",
      "\n",
      "Reward: (0, 0)\n",
      ". . . B* .\n",
      "A . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "Ball: B\n",
      "\n",
      "Reward: (0, 0)\n",
      "A . B* . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "Ball: B\n",
      "\n",
      "Reward: (0, 0)\n",
      ". B* . . .\n",
      "A . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "Ball: B\n",
      "\n",
      "Reward: (0, 0)\n",
      "B* . . . .\n",
      ". . . . .\n",
      "A . . . .\n",
      ". . . . .\n",
      "Ball: B\n",
      "\n",
      "Reward: (-1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "\n",
    "class SoccerEnv:\n",
    "    def __init__(self, grid_size=(4, 5), gamma=0.9, draw_prob=0.1):\n",
    "        self.rows, self.cols = grid_size\n",
    "        self.gamma = gamma\n",
    "        self.draw_prob = draw_prob\n",
    "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT', 'STAY']\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Initial positions (Figure 6.1-like setup)\n",
    "        # You can tweak starting coordinates if you have the figure\n",
    "        self.pos_A = [2, 1]\n",
    "        self.pos_B = [2, 3]\n",
    "        self.ball_owner = random.choice(['A', 'B'])\n",
    "        self.done = False\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # State = (A_row, A_col, B_row, B_col, ball_owner)\n",
    "        return (self.pos_A[0], self.pos_A[1],\n",
    "                self.pos_B[0], self.pos_B[1],\n",
    "                0 if self.ball_owner == 'A' else 1)\n",
    "\n",
    "    def _move(self, pos, action):\n",
    "        r, c = pos\n",
    "        if action == 'UP':\n",
    "            r = max(0, r - 1)\n",
    "        elif action == 'DOWN':\n",
    "            r = min(self.rows - 1, r + 1)\n",
    "        elif action == 'LEFT':\n",
    "            c = max(0, c - 1)\n",
    "        elif action == 'RIGHT':\n",
    "            c = min(self.cols - 1, c + 1)\n",
    "        # STAY means do nothing\n",
    "        return [r, c]\n",
    "\n",
    "    def step(self, action_A, action_B):\n",
    "        \"\"\"Executes both players’ actions in random order.\"\"\"\n",
    "        if self.done:\n",
    "            raise ValueError(\"Episode has ended. Call reset().\")\n",
    "\n",
    "        # Randomize execution order\n",
    "        order = random.choice([('A', 'B'), ('B', 'A')])\n",
    "\n",
    "        for agent in order:\n",
    "            if agent == 'A':\n",
    "                new_pos = self._move(self.pos_A, self.actions[action_A])\n",
    "                # Collision logic\n",
    "                if new_pos == self.pos_B:\n",
    "                    if self.ball_owner == 'A':\n",
    "                        self.ball_owner = 'B'  # Lose the ball\n",
    "                    # Stay in place\n",
    "                else:\n",
    "                    self.pos_A = new_pos\n",
    "            else:  # Agent B\n",
    "                new_pos = self._move(self.pos_B, self.actions[action_B])\n",
    "                if new_pos == self.pos_A:\n",
    "                    if self.ball_owner == 'B':\n",
    "                        self.ball_owner = 'A'\n",
    "                else:\n",
    "                    self.pos_B = new_pos\n",
    "\n",
    "        reward_A, reward_B = 0, 0\n",
    "\n",
    "        # Check goal conditions\n",
    "        if self.ball_owner == 'A' and self.pos_A[1] == self.cols - 1:\n",
    "            reward_A, reward_B = 1, -1\n",
    "            self.done = True\n",
    "        elif self.ball_owner == 'B' and self.pos_B[1] == 0:\n",
    "            reward_A, reward_B = -1, 1\n",
    "            self.done = True\n",
    "        elif random.random() < self.draw_prob:\n",
    "            # Draw termination\n",
    "            reward_A = reward_B = 0\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_state(), (reward_A, reward_B), self.done\n",
    "\n",
    "    def render(self):\n",
    "        grid = [['.' for _ in range(self.cols)] for _ in range(self.rows)]\n",
    "        a_r, a_c = self.pos_A\n",
    "        b_r, b_c = self.pos_B\n",
    "        grid[a_r][a_c] = 'A*' if self.ball_owner == 'A' else 'A'\n",
    "        grid[b_r][b_c] = 'B*' if self.ball_owner == 'B' else 'B'\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        print(f\"Ball: {self.ball_owner}\\n\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    env = SoccerEnv()\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        aA = np.random.randint(env.n_actions)\n",
    "        aB = np.random.randint(env.n_actions)\n",
    "        s, r, done = env.step(aA, aB)\n",
    "        env.render()\n",
    "        print(\"Reward:\", r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a5a631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc358f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
