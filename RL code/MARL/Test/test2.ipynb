{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6409730a",
   "metadata": {},
   "source": [
    "## Simple Chase example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1bfe318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pettingzoo.farama.org/environments/mpe/simple_push/\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "\n",
    "env = simple_tag_v3.env(num_good=1, num_adversaries=1 , render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "verbose = False\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    if verbose == True:\n",
    "\n",
    "        print(\"Agent: \", agent)\n",
    "        print(\"===============================================\")\n",
    "        print(\"Observation: \", observation)\n",
    "        print(\"===============================================\")\n",
    "        print(\"Action:\", action)\n",
    "        print(\"===============================================\")\n",
    "        print(\"reward: \", reward)\n",
    "        print(\"===============================================\")\n",
    "\n",
    "    \n",
    "\n",
    "    env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ce5149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adversary_0', 'agent_0']\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.34683114,  0.80838966,  0.8542746 ], dtype=float32), 'agent_0': array([-0.38499805, -1.2011057 , -0.38499805, -1.2011057 , -0.80838966,\n",
      "       -0.8542746 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.5473136829039504, 'agent_0': -0.7139866281272493})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.29683113,  0.80838966,  0.9042746 ], dtype=float32), 'agent_0': array([-0.38499805, -1.2011057 , -0.38499805, -1.2011057 , -0.80838966,\n",
      "       -0.9042746 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.5170775131871352, 'agent_0': -0.7442227978440645})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.20933113,  0.75838965,  0.9917746 ], dtype=float32), 'agent_0': array([-0.33499807, -1.2011057 , -0.33499807, -1.2011057 , -0.75838965,\n",
      "       -0.9917746 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(4), 'agent_0': np.int64(3)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.4723134098838689, 'agent_0': -0.7746343307878638})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.19370613,  0.7208896 ,  0.9573996 ], dtype=float32), 'agent_0': array([-0.29749808, -1.1511058 , -0.29749808, -1.1511058 , -0.7208896 ,\n",
      "       -0.9573996 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(4), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.46559907647618154, 'agent_0': -0.723328797892397})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.23198737,  0.74276465,  0.8816183 ], dtype=float32), 'agent_0': array([-0.31937307, -1.1136057 , -0.31937307, -1.1136057 , -0.74276465,\n",
      "       -0.8816183 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(4), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.48278212433807954, 'agent_0': -0.6757155524312072})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.42339158, -0.31069833,  0.7591709 ,  0.7747824 ], dtype=float32), 'agent_0': array([-0.3357793, -1.0854807, -0.3357793, -1.0854807, -0.7591709,\n",
      "       -0.7747824], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(2), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.525160812858757, 'agent_0': -0.6110681052887186})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.3733916 , -0.36973152,  0.7214756 ,  0.6946555 ], dtype=float32), 'agent_0': array([-0.348084 , -1.064387 , -0.348084 , -1.064387 , -0.7214756,\n",
      "       -0.6946555], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(1), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.5254737608568437, 'agent_0': -0.5943843136652033})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.3858916, -0.4140064,  0.6932041,  0.6345602], dtype=float32), 'agent_0': array([-0.30731252, -1.0485667 , -0.30731252, -1.0485667 , -0.6932041 ,\n",
      "       -0.6345602 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(4), 'agent_0': np.int64(3)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.5659625727931983, 'agent_0': -0.5267098534639973})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.3952666 , -0.4972126 ,  0.67200047,  0.48948884], dtype=float32), 'agent_0': array([-0.2767339 , -0.9867014 , -0.2767339 , -0.9867014 , -0.67200047,\n",
      "       -0.48948884], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(0), 'agent_0': np.int64(3)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.6351818971749154, 'agent_0': -0.3895919138802525})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.40229782, -0.5596172 ,  0.65609777,  0.33068526], dtype=float32), 'agent_0': array([-0.25379995, -0.8903025 , -0.25379995, -0.8903025 , -0.65609777,\n",
      "       -0.33068526], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(0), 'agent_0': np.int64(4)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.6892133156868342, 'agent_0': -0.23655823228842865})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.4075713 , -0.6064207 ,  0.64417076,  0.2615826 ], dtype=float32), 'agent_0': array([-0.23659948, -0.8680033 , -0.23659948, -0.8680033 , -0.64417076,\n",
      "       -0.2615826 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(1), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.7306575226324428, 'agent_0': -0.16901411497514995})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.46152636, -0.6415233 ,  0.7352255 ,  0.2097556 ], dtype=float32), 'agent_0': array([-0.27369913, -0.8512789 , -0.27369913, -0.8512789 , -0.7352255 ,\n",
      "       -0.2097556 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(4), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.7902902837682868, 'agent_0': -0.10390600197663691})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.50199264, -0.71785027,  0.7535165 ,  0.12088535], dtype=float32), 'agent_0': array([-0.25152385, -0.83873564, -0.25152385, -0.83873564, -0.7535165 ,\n",
      "       -0.12088535], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.8759598365730255, 'agent_0': 0.000321963312686524})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.5323424 , -0.7250955 ,  0.7172348 ,  0.10423266], dtype=float32), 'agent_0': array([-0.18489242, -0.8293281 , -0.18489242, -0.8293281 , -0.7172348 ,\n",
      "       -0.10423266], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.8995286986372396, 'agent_0': 0.04984030116570093})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.5551047 , -0.6805294 ,  0.69002354,  0.14174314], dtype=float32), 'agent_0': array([-0.13491882, -0.82227254, -0.13491882, -0.82227254, -0.69002354,\n",
      "       -0.14174314], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(2), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.8782149354261759, 'agent_0': 0.04494714096765717})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.52217644, -0.6471048 ,  0.6196151 ,  0.16987601], dtype=float32), 'agent_0': array([-0.09743863, -0.81698084, -0.09743863, -0.81698084, -0.6196151 ,\n",
      "       -0.16987601], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.8315123964031518, 'agent_0': 0.008741484764053342})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.4974802 , -0.5720364 ,  0.6168087 ,  0.24097565], dtype=float32), 'agent_0': array([-0.11932849, -0.81301206, -0.11932849, -0.81301206, -0.6168087 ,\n",
      "       -0.24097565], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.7580977528401887, 'agent_0': -0.06362476450814003})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.47895804, -0.46573508,  0.61470395,  0.3443004 ], dtype=float32), 'agent_0': array([-0.13574588, -0.81003547, -0.13574588, -0.81003547, -0.61470395,\n",
      "       -0.3443004 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(1), 'agent_0': np.int64(0)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.66806435659277, 'agent_0': -0.15326652437241395})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.51506644, -0.3860091 ,  0.6631254 ,  0.42179394], dtype=float32), 'agent_0': array([-0.14805894, -0.80780303, -0.14805894, -0.80780303, -0.6631254 ,\n",
      "       -0.42179394], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(0), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.6436586480010066, 'agent_0': -0.17760086908283235})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.5421477, -0.3262146,  0.6494414,  0.4799141], dtype=float32), 'agent_0': array([-0.10729372, -0.8061287 , -0.10729372, -0.8061287 , -0.6494414 ,\n",
      "       -0.4799141 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(2), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.6327243649147697, 'agent_0': -0.18051326174064608})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.5124587 , -0.28136873,  0.6391785 ,  0.52350426], dtype=float32), 'agent_0': array([-0.1267198 , -0.804873  , -0.1267198 , -0.804873  , -0.6391785 ,\n",
      "       -0.52350426], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(2), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.5846214787375595, 'agent_0': -0.23016585280384727})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.44019192, -0.24773434,  0.6314813 ,  0.5561968 ], dtype=float32), 'agent_0': array([-0.19128937, -0.8039312 , -0.19128937, -0.8039312 , -0.6314813 ,\n",
      "       -0.5561968 ], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(0), 'agent_0': np.int64(3)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.505115057073559, 'agent_0': -0.3212607187043024})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.38599184, -0.22250853,  0.6257084 ,  0.53071624], dtype=float32), 'agent_0': array([-0.23971654, -0.7532248 , -0.23971654, -0.7532248 , -0.6257084 ,\n",
      "       -0.53071624], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(2), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.4455330978485435, 'agent_0': -0.34491716340929096})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.29534176, -0.20358919,  0.5213787 ,  0.51160586], dtype=float32), 'agent_0': array([-0.22603692, -0.715195  , -0.22603692, -0.715195  , -0.5213787 ,\n",
      "       -0.51160586], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(3), 'agent_0': np.int64(2)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.358713417993204, 'agent_0': -0.39135099282457864})\n",
      "===============================================\n",
      "Observation:  {'adversary_0': array([ 0.22735423, -0.13939966,  0.49313143,  0.54727304], dtype=float32), 'agent_0': array([-0.2657772 , -0.6866727 , -0.2657772 , -0.6866727 , -0.49313143,\n",
      "       -0.54727304], dtype=float32)}\n",
      "===============================================\n",
      "Action: {'adversary_0': np.int64(0), 'agent_0': np.int64(1)}\n",
      "===============================================\n",
      "reward:  defaultdict(<class 'int'>, {'adversary_0': -0.2666874780842228, 'agent_0': -0.4696255845523323})\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "\n",
    "env = simple_adversary_v3.parallel_env(render_mode=\"human\", N=1)\n",
    "observations, infos = env.reset()\n",
    "verbose = True\n",
    "print(env.agents)\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    \n",
    "    if verbose == True:\n",
    "\n",
    "        # print(\"Agent: \", agent)\n",
    "        # print(\"===============================================\")\n",
    "        print(\"Observation: \", observations)\n",
    "        print(\"===============================================\")\n",
    "        print(\"Action:\", actions)\n",
    "        print(\"===============================================\")\n",
    "        print(\"reward: \", rewards)\n",
    "        print(\"===============================================\")\n",
    "\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adversary_0', 'agent_0']\n"
     ]
    }
   ],
   "source": [
    "env = simple_adversary_v3.parallel_env(render_mode=\"human\", N=1)\n",
    "env.reset()\n",
    "print(env.agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af125971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = simple_tag_v3.env(num_good=1, num_adversaries=1, render_mode=\"rgb_array\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "\n",
    "# --- Independent Q-Learning (IQL) example skeleton ---\n",
    "alpha = 0.01\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay_steps = 80000\n",
    "decay_rate = (epsilon - epsilon_min) / epsilon_decay_steps\n",
    "\n",
    "Q_tables = {agent: defaultdict(lambda: np.zeros(env.action_space(agent).n))\n",
    "            for agent in env.agents}\n",
    "\n",
    "def get_state(obs):\n",
    "    return tuple(np.round(obs, 1))  # discretize continuous obs\n",
    "\n",
    "returns = []\n",
    "for t in tqdm(range(10)):\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    done = {a: False for a in env.agents}\n",
    "\n",
    "    while not all(done.values()):\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, term, trunc, info = env.last()\n",
    "            done[agent] = term or trunc\n",
    "            if done[agent]:\n",
    "                env.step(None)\n",
    "                continue\n",
    "            state = get_state(obs)\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space(agent).sample()\n",
    "            else:\n",
    "                action = np.argmax(Q_tables[agent][state])\n",
    "\n",
    "            env.step(action)\n",
    "            next_obs, reward, term, trunc, _ = env.last()\n",
    "            next_state = get_state(next_obs)\n",
    "            done[agent] = term or trunc\n",
    "\n",
    "            best_next = np.max(Q_tables[agent][next_state])\n",
    "            Q_tables[agent][state][action] += alpha * (reward + gamma * best_next - Q_tables[agent][state][action])\n",
    "            total_reward += reward\n",
    "\n",
    "    returns.append(total_reward)\n",
    "\n",
    "    # Decay epsilon\n",
    "    if t < epsilon_decay_steps:\n",
    "        epsilon -= decay_rate\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "# Plot smoothed curve\n",
    "window = 1000\n",
    "smoothed = np.convolve(returns, np.ones(window)/window, mode='valid')\n",
    "plt.plot(smoothed)\n",
    "plt.xlabel('Environment time steps')\n",
    "plt.ylabel('Evaluation returns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0b5bd",
   "metadata": {},
   "source": [
    "## Joint Action Learning with Agent Modeling using Deep Q-Networks (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58db26b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import collections\n",
    "\n",
    "env = simple_tag_v3.parallel_env(num_good=1, num_adversaries=1, render_mode=None)\n",
    "env.reset(seed=42)\n",
    "\n",
    "alpha = 0.01\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay_steps = 80000\n",
    "decay_rate = (epsilon - epsilon_min) / epsilon_decay_steps\n",
    "episodes = 1000\n",
    "BUFFER_CAPACITY = 1000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99       # Discount factor\n",
    "TAU = 0.005        # For soft target network updates\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fde6e736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        , -0.74377275, -0.09922812,  1.0027299 ,\n",
       "        0.680199  ,  0.6419183 , -0.39174217,  0.4853688 ,  0.9527581 ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.25840396,\n",
       "        0.85353   ,  0.51736116, -0.27255908,  0.15654951, -1.3445003 ,\n",
       "       -0.4853688 , -0.9527581 ], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env.observation_space('adversary_0').shape[0]\n",
    "obs, info = env.reset()\n",
    "np.concat((obs['adversary_0'], obs['agent_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26c220c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deep Q-Network (DQN)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ModelNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.net(x), dim=-1)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[idx] for idx in batch])\n",
    "        return (np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- Joint Action Learning with Agent Modeling using Deep Q-Networks (DQN) ---\n",
    "\n",
    "global_state_dim = env.observation_space('adversary_0').shape[0] + env.observation_space('agent_0').shape[0]\n",
    "action_dim_0 = env.action_space('agent_0').n\n",
    "action_dim_1 = env.action_space('adversary_0').n\n",
    "joint_action_dim = action_dim_0 * action_dim_1\n",
    "\n",
    "\n",
    "\n",
    "# --- Agent 0 (Agent_0) ---\n",
    "# Q-Net learns Q_0(s, a_0, a_1)\n",
    "# agent_net = QNetwork(global_state_dim, joint_action_dim)\n",
    "# agent_target_net = QNetwork(global_state_dim, joint_action_dim)\n",
    "# agent_target_net.load_state_dict(agent_net.state_dict())\n",
    "\n",
    "# Model-Net learns pi_1(a_1 | s)\n",
    "# It models Agent 1 (the opponent)\n",
    "# agent_model = ModelNetwork(global_state_dim, action_dim_1) \n",
    "\n",
    "# --- Agent 0 (Agent_0) ---\n",
    "agent_net = QNetwork(global_state_dim, joint_action_dim).to(device)\n",
    "agent_target_net = QNetwork(global_state_dim, joint_action_dim).to(device)\n",
    "agent_target_net.load_state_dict(agent_net.state_dict())\n",
    "agent_model = ModelNetwork(global_state_dim, action_dim_1).to(device) \n",
    "\n",
    "\n",
    "\n",
    "# --- Agent 1 (Adversary_0) ---\n",
    "# Q-Net learns Q_1(s, a_0, a_1)\n",
    "# adversary_net = QNetwork(global_state_dim, joint_action_dim)\n",
    "# adversary_target_net = QNetwork(global_state_dim, joint_action_dim)\n",
    "# adversary_target_net.load_state_dict(adversary_net.state_dict())\n",
    "\n",
    "# Model-Net learns pi_0(a_0 | s)\n",
    "# It models Agent 0 (the opponent)\n",
    "# adversary_model = ModelNetwork(global_state_dim, action_dim_0)\n",
    "\n",
    "adversary_net = QNetwork(global_state_dim, joint_action_dim).to(device)\n",
    "adversary_target_net = QNetwork(global_state_dim, joint_action_dim).to(device)\n",
    "adversary_target_net.load_state_dict(adversary_net.state_dict())\n",
    "adversary_model = ModelNetwork(global_state_dim, action_dim_0).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "agent_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "adversary_buffer = ReplayBuffer(BUFFER_CAPACITY)\n",
    "\n",
    "q_optimizer_0 = torch.optim.Adam(agent_net.parameters(), lr=alpha)\n",
    "q_optimizer_1 = torch.optim.Adam(adversary_net.parameters(), lr=alpha)\n",
    "\n",
    "model_optimizer_0 = torch.optim.Adam(agent_model.parameters(), lr=alpha)\n",
    "model_optimizer_1 = torch.optim.Adam(adversary_model.parameters(), lr=alpha)\n",
    "\n",
    "model_loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d41cd851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:31<00:00,  3.01it/s, Avg_M_Loss=1.6480, Avg_Q_Loss=3.4614, Avg_Rwd_A0=-12.86, Avg_Rwd_A1=0.70, Epsilon=0.988]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "recent_rewards_agent_0 = collections.deque(maxlen=100)\n",
    "recent_rewards_agent_1 = collections.deque(maxlen=100)\n",
    "progress_bar = tqdm(range(episodes))\n",
    "for episode in progress_bar:\n",
    "    episode_reward_agent_0 = 0\n",
    "    episode_reward_agent_1 = 0\n",
    "    cumulative_q_loss_0 = 0\n",
    "    cumulative_q_loss_1 = 0\n",
    "    cumulative_model_loss_0 = 0\n",
    "    cumulative_model_loss_1 = 0\n",
    "    \n",
    "    episode_steps = 0\n",
    "    num_train_steps = 0 # To average loss correctly\n",
    "\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "    while not done:\n",
    "        global_state = np.concatenate((obs['adversary_0'], obs['agent_0']))\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(global_state).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            # Get Q_0(s, a_0, a_1) for ALL joint actions\n",
    "            # Output shape: [1, joint_action_dim]\n",
    "            all_joint_q_agent = agent_net(state_tensor) \n",
    "\n",
    "            # Get pi_1(a_1 | s) -> Agent 0's model of Agent 1\n",
    "            # Output shape: [1, action_dim_1]\n",
    "            probs_opponent_1 = agent_model(state_tensor) \n",
    "\n",
    "            # Reshape Q-values into a matrix: [action_dim_0, action_dim_1]\n",
    "            # Each row 'a0' contains Q-values for all opponent actions 'a1'\n",
    "            q_matrix_agent = all_joint_q_agent.view(action_dim_0, action_dim_1)\n",
    "\n",
    "            # Reshape opponent probs into a column vector: [action_dim_1, 1]\n",
    "            probs_opponent_1_vec = probs_opponent_1.view(action_dim_1, 1)\n",
    "\n",
    "            # This is Equation 6.17: AV_0 = Q_0 * pi_1\n",
    "            # (dim_0, dim_1) @ (dim_1, 1) -> (dim_0, 1)\n",
    "            action_values_agent_tensor = torch.matmul(q_matrix_agent, probs_opponent_1_vec)\n",
    "            \n",
    "            # Squeeze to get a 1D vector of action values for Agent 0\n",
    "            # Shape: [action_dim_0]\n",
    "            action_values_agent = action_values_agent_tensor.squeeze()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get Q_1(s, a_0, a_1) for ALL joint actions\n",
    "            # Output shape: [1, joint_action_dim]\n",
    "            all_joint_q_adversary = adversary_net(state_tensor)\n",
    "            \n",
    "            # Get pi_0(a_0 | s) -> Agent 1's model of Agent 0\n",
    "            # Output shape: [1, action_dim_0]\n",
    "            probs_opponent_0 = adversary_model(state_tensor)\n",
    "\n",
    "            # Reshape Q-values into a matrix: [action_dim_0, action_dim_1]\n",
    "            q_matrix_adversary = all_joint_q_adversary.view(action_dim_0, action_dim_1)\n",
    "\n",
    "            # Reshape *this* opponent's probs (Agent 0) into a column vector: [action_dim_0, 1]\n",
    "            probs_opponent_0_vec = probs_opponent_0.view(action_dim_0, 1)\n",
    "\n",
    "            # This is Equation 6.17 for Agent 1: AV_1 = (Q_1^T) * pi_0\n",
    "            # The sum is over a_0, so we must transpose the Q-matrix first.\n",
    "            # (dim_1, dim_0) @ (dim_0, 1) -> (dim_1, 1)\n",
    "            action_values_adversary_tensor = torch.matmul(q_matrix_adversary.T, probs_opponent_0_vec)\n",
    "            \n",
    "            # Squeeze to get a 1D vector of action values for Agent 1\n",
    "            # Shape: [action_dim_1]\n",
    "            action_values_adversary = action_values_adversary_tensor.squeeze()\n",
    "\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            agent_action = np.random.choice(action_dim_0)\n",
    "            adversary_action = np.random.choice(action_dim_1)\n",
    "        else:\n",
    "            agent_action = torch.argmax(action_values_agent).item()\n",
    "            adversary_action = torch.argmax(action_values_adversary).item()\n",
    "\n",
    "        actions = {'adversary_0': adversary_action, \"agent_0\": agent_action}\n",
    "        next_obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        done = all(terminations.values()) or all(truncations.values())\n",
    "        global_next_obs = np.concatenate((next_obs['adversary_0'], next_obs['agent_0']))\n",
    "        episode_reward_agent_0 += rewards['agent_0']\n",
    "        episode_reward_agent_1 += rewards['adversary_0']\n",
    "        episode_steps += 1\n",
    "\n",
    "        # === 1. TRAIN AGENT MODELS (Line 11) ===\n",
    "       \n",
    "        # We use NLLLoss (Negative Log Likelihood Loss) because your\n",
    "        # ModelNetwork outputs probabilities (after softmax).\n",
    "\n",
    "\n",
    "        # --- Train agent_model (Agent 0's model of Agent 1) ---\n",
    "        model_optimizer_0.zero_grad()\n",
    "        probs_opponent_1 = agent_model(state_tensor)\n",
    "        log_probs_1 = torch.log(probs_opponent_1 + 1e-9)\n",
    "        target_action_1 = torch.LongTensor([adversary_action]).to(device)\n",
    "\n",
    "        model_loss_0 = model_loss_fn(log_probs_1, target_action_1)\n",
    "        cumulative_model_loss_0 += model_loss_0.item()\n",
    "        model_loss_0.backward()\n",
    "        model_optimizer_0.step()\n",
    "\n",
    "\n",
    "        model_optimizer_1.zero_grad()\n",
    "        probs_opponent_0 = adversary_model(state_tensor)\n",
    "        log_probs_0 = torch.log(probs_opponent_0 + 1e-9)\n",
    "        target_action_0 = torch.LongTensor([agent_action]).to(device)\n",
    "\n",
    "        model_loss_1 = model_loss_fn(log_probs_0, target_action_0)\n",
    "        cumulative_model_loss_1 += model_loss_1.item()\n",
    "        model_loss_1.backward()\n",
    "        model_optimizer_1.step()\n",
    "\n",
    "        # =================================================================\n",
    "        # === 2. UPDATE BUFFERS (Line 9) ===\n",
    "        # We must convert the two separate actions into one joint_action index\n",
    "        # to store in the buffer, as this is what our Q-network learns.\n",
    "        # Formula: (a0 * num_actions_1) + a1\n",
    "        joint_action = agent_action * action_dim_1 + adversary_action\n",
    "        \n",
    "        agent_buffer.push(global_state, joint_action, rewards['agent_0'], global_next_obs, done)\n",
    "        \n",
    "        adversary_buffer.push(global_state, joint_action, rewards['adversary_0'], global_next_obs, done)\n",
    "\n",
    "        # =================================================================\n",
    "        # === 3. TRAIN Q-NETWORKS (Line 12) ===\n",
    "        # This is the Bellman update, using the AV(s') as the target.\n",
    "        \n",
    "        q_loss_fn = nn.MSELoss()\n",
    "\n",
    "        if len(agent_buffer) >= BATCH_SIZE:\n",
    "            states_b, joint_actions_b, rewards_b, next_states_b, dones_b = agent_buffer.sample(BATCH_SIZE)\n",
    "            \n",
    "            states_b = torch.FloatTensor(states_b).to(device)\n",
    "            joint_actions_b = torch.LongTensor(joint_actions_b).unsqueeze(1).to(device) # Shape: [B, 1]\n",
    "            rewards_b = torch.FloatTensor(rewards_b).unsqueeze(1).to(device)      # Shape: [B, 1]\n",
    "            next_states_b = torch.FloatTensor(next_states_b).to(device)\n",
    "            dones_b = torch.FloatTensor(dones_b).unsqueeze(1).to(device)          # Shape: [B, 1]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q_next_target_0 = agent_target_net(next_states_b) # [B, joint_action_dim]\n",
    "                # Get pi_1(a_1 | s') from the *opponent model*\n",
    "                model_next_1 = agent_model(next_states_b) # [B, action_dim_1]\n",
    "                \n",
    "                # Reshape for batch matrix multiplication\n",
    "                q_matrix_next_0 = q_next_target_0.view(BATCH_SIZE, action_dim_0, action_dim_1)\n",
    "                model_vec_next_1 = model_next_1.unsqueeze(2) # Shape: [B, action_dim_1, 1]\n",
    "                \n",
    "                # AV_0(s', a_0) = Q_target_0 * pi_1\n",
    "                av_next_0 = torch.bmm(q_matrix_next_0, model_vec_next_1) # Shape: [B, action_dim_0, 1]\n",
    "                \n",
    "                # max_a0 AV_0(s', a_0)\n",
    "                max_av_next_0, _ = torch.max(av_next_0, dim=1) # Shape: [B, 1]\n",
    "                \n",
    "                # Bellman Target: y_0 = r + gamma * max_AV * (1 - done)\n",
    "                target_q_0 = rewards_b + (GAMMA * max_av_next_0 * (1 - dones_b))\n",
    "\n",
    "            # --- Get Current Q-value: Q_0(s, a) ---\n",
    "            q_current_0 = agent_net(states_b) # [B, joint_action_dim]\n",
    "            # Get Q_0 for the *specific* joint action taken from the buffer\n",
    "            q_current_0_selected = q_current_0.gather(1, joint_actions_b) # [B, 1]\n",
    "            \n",
    "            # --- Calculate Loss and Optimize ---\n",
    "            q_loss_0 = q_loss_fn(q_current_0_selected, target_q_0)\n",
    "            q_optimizer_0.zero_grad()\n",
    "            cumulative_q_loss_0 += q_loss_0.item()\n",
    "            num_train_steps += 1 # Increment this *only* when you train\n",
    "            q_loss_0.backward()\n",
    "            q_optimizer_0.step()\n",
    "\n",
    "            # --- Soft update target network ---\n",
    "            for target_param, local_param in zip(agent_target_net.parameters(), agent_net.parameters()):\n",
    "                target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)\n",
    "            \n",
    "        # --- Train Agent 1's Q-Network (Adversary) ---\n",
    "        if len(adversary_buffer) >= BATCH_SIZE:\n",
    "            # Sample a batch\n",
    "            states_b, joint_actions_b, rewards_b, next_states_b, dones_b = adversary_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            # Convert to Tensors\n",
    "            states_b = torch.FloatTensor(states_b).to(device)\n",
    "            joint_actions_b = torch.LongTensor(joint_actions_b).unsqueeze(1).to(device)\n",
    "            rewards_b = torch.FloatTensor(rewards_b).unsqueeze(1).to(device)\n",
    "            next_states_b = torch.FloatTensor(next_states_b).to(device)\n",
    "            dones_b = torch.FloatTensor(dones_b).unsqueeze(1).to(device)\n",
    "\n",
    "            # --- Calculate Target y_1 ---\n",
    "            # y_1 = r + gamma * max_a1' AV_1(s', a_1')\n",
    "            with torch.no_grad():\n",
    "                # Get Q_target_1(s', a_0, a_1) from the *target network*\n",
    "                q_next_target_1 = adversary_target_net(next_states_b) # [B, joint_action_dim]\n",
    "                # Get pi_0(a_0 | s') from the *opponent model*\n",
    "                model_next_0 = adversary_model(next_states_b) # [B, action_dim_0]\n",
    "                \n",
    "                # Reshape for batch matrix multiplication\n",
    "                q_matrix_next_1 = q_next_target_1.view(BATCH_SIZE, action_dim_0, action_dim_1)\n",
    "                model_vec_next_0 = model_next_0.unsqueeze(2) # [B, action_dim_0, 1]\n",
    "                \n",
    "                # AV_1(s', a_1) = (Q_target_1^T) * pi_0\n",
    "                # We transpose the Q-matrix to sum over a_0\n",
    "                av_next_1 = torch.bmm(q_matrix_next_1.transpose(1, 2), model_vec_next_0) # [B, action_dim_1, 1]\n",
    "                \n",
    "                # max_a1 AV_1(s', a_1)\n",
    "                max_av_next_1, _ = torch.max(av_next_1, dim=1) # [B, 1]\n",
    "                \n",
    "                # Bellman Target: y_1 = r + gamma * max_AV * (1 - done)\n",
    "                target_q_1 = rewards_b + (GAMMA * max_av_next_1 * (1 - dones_b))\n",
    "                \n",
    "            # --- Get Current Q-value: Q_1(s, a) ---\n",
    "            q_current_1 = adversary_net(states_b)\n",
    "            q_current_1_selected = q_current_1.gather(1, joint_actions_b)\n",
    "            \n",
    "            # --- Calculate Loss and Optimize ---\n",
    "            q_loss_1 = q_loss_fn(q_current_1_selected, target_q_1)\n",
    "            q_optimizer_1.zero_grad()\n",
    "            cumulative_q_loss_1 += q_loss_1.item()\n",
    "            q_loss_1.backward()\n",
    "            q_optimizer_1.step()\n",
    "\n",
    "            # --- Soft update target network ---\n",
    "            for target_param, local_param in zip(adversary_target_net.parameters(), adversary_net.parameters()):\n",
    "                target_param.data.copy_(TAU * local_param.data + (1.0 - TAU) * target_param.data)\n",
    "\n",
    "        # =================================================================\n",
    "        # === 4. PREPARE FOR NEXT STEP ===\n",
    "        obs = next_obs\n",
    "        global_state = global_next_obs\n",
    "    \n",
    "    # --- (End of `while not done` loop) ---\n",
    "    avg_q_loss_0 = cumulative_q_loss_0 / num_train_steps if num_train_steps > 0 else 0\n",
    "    avg_q_loss_1 = cumulative_q_loss_1 / num_train_steps if num_train_steps > 0 else 0\n",
    "    \n",
    "    avg_model_loss_0 = cumulative_model_loss_0 / episode_steps if episode_steps > 0 else 0\n",
    "    avg_model_loss_1 = cumulative_model_loss_1 / episode_steps if episode_steps > 0 else 0\n",
    "\n",
    "    recent_rewards_agent_0.append(episode_reward_agent_0)\n",
    "    recent_rewards_agent_1.append(episode_reward_agent_1)\n",
    "    \n",
    "    avg_reward_0 = sum(recent_rewards_agent_0) / len(recent_rewards_agent_0)\n",
    "    avg_reward_1 = sum(recent_rewards_agent_1) / len(recent_rewards_agent_1)\n",
    "    \n",
    "    progress_bar.set_postfix(\n",
    "        Avg_Rwd_A0=f\"{avg_reward_0:.2f}\",\n",
    "        Avg_Rwd_A1=f\"{avg_reward_1:.2f}\",\n",
    "        Epsilon=f\"{epsilon:.3f}\",\n",
    "        Avg_Q_Loss=f\"{avg_q_loss_0:.4f}\", # Added a bit more precision\n",
    "        Avg_M_Loss=f\"{avg_model_loss_0:.4f}\"\n",
    "    )\n",
    "    # tqdm.write(desc) # Use tqdm.write instead of print to avoid breaking the bar\n",
    "\n",
    "    # Decay epsilon (your existing code)\n",
    "    if episode < epsilon_decay_steps:\n",
    "        epsilon -= decay_rate\n",
    "        epsilon = max(epsilon, epsilon_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661eb978",
   "metadata": {},
   "source": [
    "## Soccer Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . .\n",
      ". . . B* .\n",
      "A . . . .\n",
      ". . . . .\n",
      "Ball: B\n",
      "\n",
      "Reward: (0, 0)\n",
      ". . . B* .\n",
      "A . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "Ball: B\n",
      "\n",
      "Reward: (0, 0)\n",
      "A . B* . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "Ball: B\n",
      "\n",
      "Reward: (0, 0)\n",
      ". B* . . .\n",
      "A . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "Ball: B\n",
      "\n",
      "Reward: (0, 0)\n",
      "B* . . . .\n",
      ". . . . .\n",
      "A . . . .\n",
      ". . . . .\n",
      "Ball: B\n",
      "\n",
      "Reward: (-1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "\n",
    "class SoccerEnv:\n",
    "    def __init__(self, grid_size=(4, 5), gamma=0.9, draw_prob=0.1):\n",
    "        self.rows, self.cols = grid_size\n",
    "        self.gamma = gamma\n",
    "        self.draw_prob = draw_prob\n",
    "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT', 'STAY']\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Initial positions (Figure 6.1-like setup)\n",
    "        # You can tweak starting coordinates if you have the figure\n",
    "        self.pos_A = [2, 1]\n",
    "        self.pos_B = [2, 3]\n",
    "        self.ball_owner = random.choice(['A', 'B'])\n",
    "        self.done = False\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # State = (A_row, A_col, B_row, B_col, ball_owner)\n",
    "        return (self.pos_A[0], self.pos_A[1],\n",
    "                self.pos_B[0], self.pos_B[1],\n",
    "                0 if self.ball_owner == 'A' else 1)\n",
    "\n",
    "    def _move(self, pos, action):\n",
    "        r, c = pos\n",
    "        if action == 'UP':\n",
    "            r = max(0, r - 1)\n",
    "        elif action == 'DOWN':\n",
    "            r = min(self.rows - 1, r + 1)\n",
    "        elif action == 'LEFT':\n",
    "            c = max(0, c - 1)\n",
    "        elif action == 'RIGHT':\n",
    "            c = min(self.cols - 1, c + 1)\n",
    "        # STAY means do nothing\n",
    "        return [r, c]\n",
    "\n",
    "    def step(self, action_A, action_B):\n",
    "        \"\"\"Executes both players’ actions in random order.\"\"\"\n",
    "        if self.done:\n",
    "            raise ValueError(\"Episode has ended. Call reset().\")\n",
    "\n",
    "        # Randomize execution order\n",
    "        order = random.choice([('A', 'B'), ('B', 'A')])\n",
    "\n",
    "        for agent in order:\n",
    "            if agent == 'A':\n",
    "                new_pos = self._move(self.pos_A, self.actions[action_A])\n",
    "                # Collision logic\n",
    "                if new_pos == self.pos_B:\n",
    "                    if self.ball_owner == 'A':\n",
    "                        self.ball_owner = 'B'  # Lose the ball\n",
    "                    # Stay in place\n",
    "                else:\n",
    "                    self.pos_A = new_pos\n",
    "            else:  # Agent B\n",
    "                new_pos = self._move(self.pos_B, self.actions[action_B])\n",
    "                if new_pos == self.pos_A:\n",
    "                    if self.ball_owner == 'B':\n",
    "                        self.ball_owner = 'A'\n",
    "                else:\n",
    "                    self.pos_B = new_pos\n",
    "\n",
    "        reward_A, reward_B = 0, 0\n",
    "\n",
    "        # Check goal conditions\n",
    "        if self.ball_owner == 'A' and self.pos_A[1] == self.cols - 1:\n",
    "            reward_A, reward_B = 1, -1\n",
    "            self.done = True\n",
    "        elif self.ball_owner == 'B' and self.pos_B[1] == 0:\n",
    "            reward_A, reward_B = -1, 1\n",
    "            self.done = True\n",
    "        elif random.random() < self.draw_prob:\n",
    "            # Draw termination\n",
    "            reward_A = reward_B = 0\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_state(), (reward_A, reward_B), self.done\n",
    "\n",
    "    def render(self):\n",
    "        grid = [['.' for _ in range(self.cols)] for _ in range(self.rows)]\n",
    "        a_r, a_c = self.pos_A\n",
    "        b_r, b_c = self.pos_B\n",
    "        grid[a_r][a_c] = 'A*' if self.ball_owner == 'A' else 'A'\n",
    "        grid[b_r][b_c] = 'B*' if self.ball_owner == 'B' else 'B'\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        print(f\"Ball: {self.ball_owner}\\n\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    env = SoccerEnv()\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        aA = np.random.randint(env.n_actions)\n",
    "        aB = np.random.randint(env.n_actions)\n",
    "        s, r, done = env.step(aA, aB)\n",
    "        env.render()\n",
    "        print(\"Reward:\", r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a5a631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc358f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
