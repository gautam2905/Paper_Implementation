{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42e2659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -------- Activation functions (as in the paper experiments) --------\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(y):\n",
    "    # y = sigmoid(x) already\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(y):\n",
    "    # y = tanh(x) already\n",
    "    return 1.0 - y**2\n",
    "\n",
    "\n",
    "# --------- LSTM Class (1997 version, no forget gate) ----------\n",
    "class LSTM1997:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # Input gate params\n",
    "        self.W_in_x = np.random.uniform(-0.1, 0.1, (hidden_size, input_size))\n",
    "        self.W_in_h = np.random.uniform(-0.1, 0.1, (hidden_size, hidden_size))\n",
    "        self.b_in = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Output gate params\n",
    "        self.W_out_x = np.random.uniform(-0.1, 0.1, (hidden_size, input_size))\n",
    "        self.W_out_h = np.random.uniform(-0.1, 0.1, (hidden_size, hidden_size))\n",
    "        self.b_out = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Cell input params\n",
    "        self.W_cell_x = np.random.uniform(-0.1, 0.1, (hidden_size, input_size))\n",
    "        self.W_cell_h = np.random.uniform(-0.1, 0.1, (hidden_size, hidden_size))\n",
    "        self.b_cell = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Final output layer\n",
    "        self.W_final = np.random.uniform(-0.1, 0.1, (output_size, hidden_size))\n",
    "        self.b_final = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" Forward pass through time.\n",
    "        X: shape (T, input_size)\n",
    "        \"\"\"\n",
    "        T = X.shape[0]\n",
    "        self.y_in = np.zeros((T, self.hidden_size, 1))\n",
    "        self.y_out = np.zeros((T, self.hidden_size, 1))\n",
    "        self.state = np.zeros((T + 1, self.hidden_size, 1))  # CEC\n",
    "        self.y_cell = np.zeros((T, self.hidden_size, 1))\n",
    "\n",
    "        self.net_in = np.zeros((T, self.hidden_size, 1))\n",
    "        self.net_out = np.zeros((T, self.hidden_size, 1))\n",
    "        self.net_cell = np.zeros((T, self.hidden_size, 1))\n",
    "\n",
    "        y_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = X[t].reshape(self.input_size, 1)\n",
    "\n",
    "            # Input gate\n",
    "            self.net_in[t] = self.W_in_x @ x_t + self.W_in_h @ y_prev + self.b_in\n",
    "            self.y_in[t] = sigmoid(self.net_in[t])\n",
    "\n",
    "            # Output gate\n",
    "            self.net_out[t] = self.W_out_x @ x_t + self.W_out_h @ y_prev + self.b_out\n",
    "            self.y_out[t] = sigmoid(self.net_out[t])\n",
    "\n",
    "            # Cell input\n",
    "            self.net_cell[t] = self.W_cell_x @ x_t + self.W_cell_h @ y_prev + self.b_cell\n",
    "\n",
    "            # CEC update (constant self-loop weight = 1.0)\n",
    "            self.state[t + 1] = self.state[t] + self.y_in[t] * tanh(self.net_cell[t])\n",
    "\n",
    "            # Cell output\n",
    "            self.y_cell[t] = self.y_out[t] * tanh(self.state[t + 1])\n",
    "\n",
    "            y_prev = self.y_cell[t]\n",
    "\n",
    "        # Final output (linear)\n",
    "        net_final = self.W_final @ self.y_cell[-1] + self.b_final\n",
    "        y_final = net_final\n",
    "        return y_final\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\" Backward pass (truncated outside the CEC) \"\"\"\n",
    "        T = X.shape[0]\n",
    "        # Loss gradient (MSE)\n",
    "        delta_final = (y_pred - y_true)\n",
    "\n",
    "        dW_final = delta_final @ self.y_cell[-1].T\n",
    "        db_final = delta_final\n",
    "\n",
    "        # Error flowing back from final layer into last cell output\n",
    "        delta_y_cell = np.zeros((T, self.hidden_size, 1))\n",
    "        delta_state = np.zeros((T + 1, self.hidden_size, 1))\n",
    "\n",
    "        delta_y_cell[T - 1] = self.W_final.T @ delta_final\n",
    "\n",
    "        # Initialize accumulators\n",
    "        dW_in_x = np.zeros_like(self.W_in_x)\n",
    "        dW_in_h = np.zeros_like(self.W_in_h)\n",
    "        db_in = np.zeros_like(self.b_in)\n",
    "\n",
    "        dW_out_x = np.zeros_like(self.W_out_x)\n",
    "        dW_out_h = np.zeros_like(self.W_out_h)\n",
    "        db_out = np.zeros_like(self.b_out)\n",
    "\n",
    "        dW_cell_x = np.zeros_like(self.W_cell_x)\n",
    "        dW_cell_h = np.zeros_like(self.W_cell_h)\n",
    "        db_cell = np.zeros_like(self.b_cell)\n",
    "\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            # Output gate gradient\n",
    "            delta_out_t = sigmoid_prime(self.y_out[t]) * tanh(self.state[t + 1]) * delta_y_cell[t]\n",
    "\n",
    "            # Error to state through output\n",
    "            delta_state[t + 1] += self.y_out[t] * tanh_prime(tanh(self.state[t + 1])) * delta_y_cell[t]\n",
    "\n",
    "            # Input gate gradient\n",
    "            delta_in_t = sigmoid_prime(self.y_in[t]) * tanh(self.net_cell[t]) * delta_state[t + 1]\n",
    "\n",
    "            # Cell input gradient\n",
    "            delta_cell_t = tanh_prime(tanh(self.net_cell[t])) * self.y_in[t] * delta_state[t + 1]\n",
    "\n",
    "            # Prepare inputs\n",
    "            x_t = X[t].reshape(self.input_size, 1)\n",
    "            y_prev = self.y_cell[t - 1] if t > 0 else np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            # Weight updates\n",
    "            dW_in_x += delta_in_t @ x_t.T\n",
    "            dW_in_h += delta_in_t @ y_prev.T\n",
    "            db_in += delta_in_t\n",
    "\n",
    "            dW_out_x += delta_out_t @ x_t.T\n",
    "            dW_out_h += delta_out_t @ y_prev.T\n",
    "            db_out += delta_out_t\n",
    "\n",
    "            dW_cell_x += delta_cell_t @ x_t.T\n",
    "            dW_cell_h += delta_cell_t @ y_prev.T\n",
    "            db_cell += delta_cell_t\n",
    "\n",
    "            # Backprop into previous cell output (BUT truncated at gate nets)\n",
    "            delta_y_prev = (self.W_in_h.T @ delta_in_t +\n",
    "                            self.W_out_h.T @ delta_out_t +\n",
    "                            self.W_cell_h.T @ delta_cell_t)\n",
    "\n",
    "            if t > 0:\n",
    "                delta_y_cell[t - 1] += delta_y_prev\n",
    "                # Constant error flow in state\n",
    "                delta_state[t] = delta_state[t + 1]\n",
    "\n",
    "        # Gradient descent update\n",
    "        self.W_in_x -= self.lr * dW_in_x\n",
    "        self.W_in_h -= self.lr * dW_in_h\n",
    "        self.b_in -= self.lr * db_in\n",
    "\n",
    "        self.W_out_x -= self.lr * dW_out_x\n",
    "        self.W_out_h -= self.lr * dW_out_h\n",
    "        self.b_out -= self.lr * db_out\n",
    "\n",
    "        self.W_cell_x -= self.lr * dW_cell_x\n",
    "        self.W_cell_h -= self.lr * dW_cell_h\n",
    "        self.b_cell -= self.lr * db_cell\n",
    "\n",
    "        self.W_final -= self.lr * dW_final\n",
    "        self.b_final -= self.lr * db_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7076a052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.19210100120351656\n",
      "Epoch 2, Train Loss: 0.18814185806595984\n",
      "Epoch 3, Train Loss: 0.18716715742268997\n",
      "Epoch 4, Train Loss: 0.1863795031375201\n",
      "Epoch 5, Train Loss: 0.18572902466322536\n"
     ]
    }
   ],
   "source": [
    "def generate_adding_dataset(num_samples, seq_length=100, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    X = np.zeros((num_samples, seq_length, 2))\n",
    "    y = np.zeros((num_samples, 1))\n",
    "    for i in range(num_samples):\n",
    "        X[i, :, 0] = np.random.uniform(0, 1, seq_length)\n",
    "        markers = np.zeros(seq_length)\n",
    "        pos1 = np.random.randint(0, seq_length // 2)\n",
    "        pos2 = np.random.randint(seq_length // 2, seq_length)\n",
    "        markers[pos1] = 1\n",
    "        markers[pos2] = 1\n",
    "        X[i, :, 1] = markers\n",
    "        y[i] = X[i, pos1, 0] + X[i, pos2, 0]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "train_X, train_y = generate_adding_dataset(1000)\n",
    "test_X, test_y = generate_adding_dataset(200)\n",
    "\n",
    "lstm = LSTM1997(input_size=2, hidden_size=10, output_size=1, learning_rate=0.05)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(train_X)):\n",
    "        X_seq = train_X[i]\n",
    "        y_true = train_y[i].reshape(1, 1)\n",
    "        y_pred = lstm.forward(X_seq)\n",
    "        loss = np.mean((y_pred - y_true)**2)\n",
    "        total_loss += loss\n",
    "        lstm.backward(X_seq, y_true, y_pred)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {total_loss / len(train_X)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a20a23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction: [[1.13314262]]\n",
      "True value: [[0.90234759]]\n"
     ]
    }
   ],
   "source": [
    "x_try, y_true = generate_adding_dataset(1)\n",
    "y_try = lstm.forward(x_try[0])\n",
    "print(\"Test prediction:\", y_try)\n",
    "print(\"True value:\", y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
