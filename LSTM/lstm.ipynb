{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d23ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(y):\n",
    "    # y = sigmoid(x) already\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(y):\n",
    "    # y = tanh(x) already\n",
    "    return 1.0 - y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42e2659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -------- Activation functions (as in the paper experiments) --------\n",
    "def f(x):\n",
    "    np.clip(x, -500, 500, out=x)  # prevent overflow\n",
    "    return 1.0 / (1.0 + np.exp(-x))  # sigmoid\n",
    "\n",
    "def h(x):\n",
    "    return 2.0 * f(x) - 1.0          # squashed to [-1, 1]\n",
    "\n",
    "def g(x):\n",
    "    return 2.0 * h(x)                # scaled version\n",
    "\n",
    "\n",
    "class LSTM1997:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # Weights initialization\n",
    "        self.W_i   = np.random.randn(hidden_size, input_size + hidden_size) * 0.1  # input gate\n",
    "        self.W_in  = np.random.randn(hidden_size, input_size + hidden_size) * 0.1  # input to cell\n",
    "        self.W_out = np.random.randn(hidden_size, input_size + hidden_size) * 0.1  # output gate\n",
    "        self.W_c   = np.random.randn(hidden_size, input_size + hidden_size) * 0.1  # memory cells\n",
    "        self.W_k   = np.random.randn(output_size, hidden_size + hidden_size) * 0.1 # outputs\n",
    "\n",
    "        # States\n",
    "        self.h_prev = np.zeros(hidden_size)  # hidden activations\n",
    "        self.s_c    = np.zeros(hidden_size)  # cell states\n",
    "        self.y_c    = np.zeros(hidden_size)  # cell outputs\n",
    "        self.caches = []                     # store all timesteps for BPTT\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through a sequence X of shape (T, input_size).\n",
    "        Returns predictions Y of shape (T, output_size).\n",
    "        \"\"\"\n",
    "        T = len(X)\n",
    "        Y = []\n",
    "\n",
    "        self.caches = []   # reset for new sequence\n",
    "        for t in range(T):\n",
    "            x_t = X[t]\n",
    "            cache = {}\n",
    "            y_prev = np.concatenate((x_t, self.h_prev))\n",
    "\n",
    "            net_i = self.W_i @ y_prev\n",
    "            y_i = f(net_i)\n",
    "\n",
    "            net_in = self.W_in @ y_prev\n",
    "            y_in = g(net_in)\n",
    "\n",
    "            net_out = self.W_out @ y_prev\n",
    "            y_out = f(net_out)\n",
    "\n",
    "            net_c = self.W_c @ y_prev\n",
    "            self.s_c = self.s_c + y_in * g(net_c)   # cell state\n",
    "            h_s = h(self.s_c)                       # squashed state\n",
    "            self.y_c = y_out * h_s                  # cell output\n",
    "\n",
    "            y_mem_hidden = np.concatenate([self.h_prev, self.y_c])\n",
    "            net_k = self.W_k @ y_mem_hidden\n",
    "            y_k = f(net_k)\n",
    "\n",
    "            self.h_prev = self.y_c   # update recurrent hidden state\n",
    "\n",
    "            # Save everything for BPTT\n",
    "            cache[\"y_prev\"] = y_prev\n",
    "            cache[\"net_i\"], cache[\"y_i\"] = net_i, y_i\n",
    "            cache[\"net_in\"], cache[\"y_in\"] = net_in, y_in\n",
    "            cache[\"net_out\"], cache[\"y_out\"] = net_out, y_out\n",
    "            cache[\"net_c\"], cache[\"g_net_c\"] = net_c, g(net_c)\n",
    "            cache[\"s_c\"], cache[\"h_s\"], cache[\"y_c\"] = self.s_c.copy(), h_s, self.y_c.copy()\n",
    "            cache[\"y_mem_hidden\"], cache[\"net_k\"], cache[\"y_k\"] = y_mem_hidden, net_k, y_k\n",
    "\n",
    "            self.caches.append(cache)\n",
    "            Y.append(y_k)\n",
    "\n",
    "        return np.array(Y)\n",
    "\n",
    "    def backward(self, X, Y_true, Y_pred, truncation=5):\n",
    "        \"\"\"\n",
    "        Backward pass with truncated BPTT.\n",
    "        X: (T, input_size)\n",
    "        Y_true: (T, output_size)\n",
    "        Y_pred: (T, output_size)\n",
    "        \"\"\"\n",
    "        T = len(X)\n",
    "\n",
    "        # Initialize weight gradients\n",
    "        dW_k   = np.zeros_like(self.W_k)\n",
    "        dW_out = np.zeros_like(self.W_out)\n",
    "        dW_in  = np.zeros_like(self.W_in)\n",
    "        dW_i   = np.zeros_like(self.W_i)\n",
    "        dW_c   = np.zeros_like(self.W_c)\n",
    "\n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(T)):\n",
    "            cache = self.caches[t]\n",
    "            y_k = cache['y_k']\n",
    "\n",
    "            # Output layer gradient\n",
    "            f_prime_k = y_k * (1 - y_k)\n",
    "            if t == len(self.caches) - 1:\n",
    "                e_k = f_prime_k * (Y_pred[0] - Y_true[0])\n",
    "            else:\n",
    "                e_k = np.zeros_like(y_k)   # no direct error from loss here\n",
    "            # e_k = f_prime_k * (Y_pred[t] - Y_true[t])\n",
    "\n",
    "            y_mem = cache['y_mem_hidden']\n",
    "            dW_k += self.lr * np.outer(e_k, y_mem)\n",
    "\n",
    "            # Propagate error into memory\n",
    "            delta_mem = self.W_k.T @ e_k\n",
    "            delta_h = delta_mem[:self.hidden_size]\n",
    "            delta_y_c = delta_mem[self.hidden_size:]\n",
    "\n",
    "            # Input gate error\n",
    "            y_i = cache['y_i']\n",
    "            f_prime_i = y_i * (1 - y_i)\n",
    "            e_i = f_prime_i * delta_h\n",
    "            y_prev = cache['y_prev']\n",
    "            dW_i += self.lr * np.outer(e_i, y_prev)\n",
    "\n",
    "            # Output gate error\n",
    "            y_out = cache['y_out']\n",
    "            f_prime_out = y_out * (1 - y_out)\n",
    "            e_out = f_prime_out * (cache['h_s'] * delta_y_c)\n",
    "            dW_out += self.lr * np.outer(e_out, y_prev)\n",
    "\n",
    "            # Cell state error\n",
    "            hprime_s = (1 - cache['h_s']**2) * 0.5\n",
    "            e_sc = y_out * hprime_s * delta_y_c\n",
    "\n",
    "            # Input to cell error\n",
    "            g_net_c = cache['g_net_c']\n",
    "            y_in = cache['y_in']\n",
    "            g_prime_c = 1 - (y_in/2.0)**2  # approx derivative\n",
    "            coeff_in = e_sc * g_net_c * g_prime_c\n",
    "            dW_in += self.lr * (coeff_in[:, None] @ y_prev[None, :])\n",
    "\n",
    "            # Cell input error\n",
    "            gprime_net_c = 1 - (g_net_c/2.0)**2\n",
    "            coeff_c = e_sc * y_in * gprime_net_c\n",
    "            dW_c += self.lr * (coeff_c[:, None] @ y_prev[None, :])\n",
    "\n",
    "            # Truncate BPTT: stop after 'truncation' steps\n",
    "            if T - t > truncation:\n",
    "                break\n",
    "\n",
    "        # Update weights\n",
    "        self.W_k   += dW_k\n",
    "        self.W_out += dW_out\n",
    "        self.W_in  += dW_in\n",
    "        self.W_i   += dW_i\n",
    "        self.W_c   += dW_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd972780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e104bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMlatest():\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        self.Wf = np.random.randn(hidden_size, input_size + hidden_size) * 0.1  # forget gate\n",
    "        self.Wi = np.random.randn(hidden_size, input_size + hidden_size) * 0.1  # input gate\n",
    "        self.Wo = np.random.randn(hidden_size, input_size + hidden_size) * 0.1  # output gate\n",
    "        self.Wc = np.random.randn(hidden_size, input_size + hidden_size) * 0.1  #   cell gate\n",
    "        self.Wy = np.random.randn(output_size, hidden_size) * 0.1\n",
    "\n",
    "        self.dWf = np.zeros_like(self.Wf)\n",
    "        self.dWi = np.zeros_like(self.Wi)\n",
    "        self.dWo = np.zeros_like(self.Wo)\n",
    "        self.dWc = np.zeros_like(self.Wc)\n",
    "        self.dWy = np.zeros_like(self.Wy)\n",
    "\n",
    "        self.bf = np.zeros(hidden_size)\n",
    "        self.bi = np.zeros(hidden_size)\n",
    "        self.bo = np.zeros(hidden_size)\n",
    "        self.bc = np.zeros(hidden_size)\n",
    "        self.by = np.zeros(output_size)\n",
    "\n",
    "        self.dbf = np.zeros_like(self.bi)\n",
    "        self.dbi = np.zeros_like(self.bi)\n",
    "        self.dbo = np.zeros_like(self.bi)\n",
    "        self.dbc = np.zeros_like(self.bi)\n",
    "        self.dby = np.zeros_like(self.by)\n",
    "\n",
    "        self.h_prev = np.zeros(hidden_size)  # hidden state\n",
    "        self.c_prev = np.zeros(hidden_size)  # cell state\n",
    "        self.caches = []                     # store all timesteps for BPTT\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        concat = np.concatenate((self.h_prev, x))  # concatenate h_prev and x\n",
    "        f_t = sigmoid(np.dot(self.Wf, concat) + self.bf)  # forget gate\n",
    "\n",
    "        i_t = sigmoid(np.dot(self.Wi, concat) + self.bi)  # input gate\n",
    "        c_tilda_t = np.tanh(np.dot(self.Wc, concat) + self.bc)  # candidate cell state\n",
    "\n",
    "        C_prev = self.c_prev\n",
    "        C_t = f_t * C_prev + i_t * c_tilda_t  # new cell state\n",
    "        self.c_prev = C_t\n",
    "\n",
    "        o_t = sigmoid(np.dot(self.Wo, concat) + self.bo)  # output gate\n",
    "        h_t = o_t * np.tanh(C_t)  # new hidden state\n",
    "        self.h_prev = h_t\n",
    "\n",
    "        self.caches.append((concat, f_t, i_t, c_tilda_t, C_t, o_t, h_t, C_prev))  # store cache for backpropagation\n",
    "        y_t = np.dot(self.Wy, h_t) + self.by\n",
    "\n",
    "        return y_t, h_t\n",
    "\n",
    "    def backward(self, dy, dh_next, dc_next):\n",
    "\n",
    "        concat, f_t, i_t, c_tilda_t, C_t, o_t, h_t, C_prev = self.caches.pop()\n",
    "\n",
    "        do_t = dh_next * np.tanh(C_t)\n",
    "        dC_total = dh_next * o_t * (1 - np.power(np.tanh(C_t), 2))\n",
    "        dc_t = dC_total + dc_next\n",
    "\n",
    "        dWy = np.outer(dy, h_t)\n",
    "        dby = dy\n",
    "        dh = np.dot(self.Wy.T, dy) + dh_next\n",
    "        self.dWy += dWy\n",
    "        self.dby += dby\n",
    "\n",
    "        df_t = dc_t * C_prev\n",
    "        di_t = dc_t * c_tilda_t\n",
    "        dc_tilda_t = dc_t * i_t\n",
    "        dc_prev = dc_t * f_t\n",
    "\n",
    "        df_t_raw = df_t * f_t * (1 - f_t)\n",
    "        di_t_raw = di_t * i_t * (1 - i_t)\n",
    "        do_t_raw = do_t * o_t * (1 - o_t)\n",
    "        dc_tilda_t_raw = dc_tilda_t * (1 - np.power(c_tilda_t, 2 ))\n",
    "\n",
    "        dWf = np.outer(df_t_raw, concat)\n",
    "        dWi = np.outer(di_t_raw, concat)\n",
    "        dWo = np.outer(do_t_raw, concat)\n",
    "        dWc = np.outer(dc_tilda_t_raw, concat)\n",
    "\n",
    "        self.dWc += dWc\n",
    "        self.dWi += dWi\n",
    "        self.dWo += dWo\n",
    "        self.dWf += dWf\n",
    "\n",
    "        dbf = df_t_raw\n",
    "        dbi = di_t_raw\n",
    "        dbc = dc_tilda_t_raw        \n",
    "        dbo = do_t_raw\n",
    "\n",
    "        self.dbf += dbf\n",
    "        self.dbo += dbo\n",
    "        self.dbc += dbc\n",
    "        self.dbi += dbi\n",
    "        \n",
    "        dconcat = (\n",
    "            np.dot(self.Wf.T, df_t_raw)\n",
    "            + np.dot(self.Wi.T, di_t_raw)\n",
    "            + np.dot(self.Wo.T, do_t_raw)\n",
    "            + np.dot(self.Wc.T, dc_tilda_t_raw)\n",
    "        )\n",
    "\n",
    "        dh_prev = dconcat[:self.hidden_size]\n",
    "        dx = dconcat[self.hidden_size:]\n",
    "\n",
    "        return dh_prev, dc_prev, dx\n",
    "\n",
    "    def update(self):\n",
    "        for param, dparam in zip(\n",
    "            [self.Wf, self.Wi, self.Wo, self.Wc, self.bf, self.bi, self.bo, self.bc, self.Wy, self.by],\n",
    "            [self.dWf, self.dWi, self.dWo, self.dWc, self.dbf, self.dbi, self.dbo, self.dbc, self.dWy, self.dby],\n",
    "        ):\n",
    "            param -= self.lr * dparam\n",
    "\n",
    "\n",
    "        for dparam in [self.dWf, self.dWi, self.dWo, self.dWc, self.dbf, self.dbi, self.dbo, self.dbc]:\n",
    "            dparam.fill(0)\n",
    "\n",
    "\n",
    "lstm = LSTMlatest(20, 10, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076a052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1115394, Char Size: 65\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"shakespeare.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "chars = sorted(list(set(data)))\n",
    "data_size, char_size = len(data), len(chars)\n",
    "print(f'Data size: {data_size}, Char Size: {char_size}')\n",
    "\n",
    "char_to_idx = {c:i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "train_X = data[:-1]\n",
    "train_y = data[1:]\n",
    "\n",
    "# One-hot encoding\n",
    "def char_to_onehot(c):\n",
    "    vec = np.zeros(len(chars))\n",
    "    vec[char_to_idx[c]] = 1\n",
    "    return vec\n",
    "\n",
    "X_seq = [char_to_onehot(c) for c in train_X]\n",
    "Y_seq = [char_to_onehot(c) for c in train_y]\n",
    "\n",
    "def sample(lstm, seed_char, length=200):\n",
    "    lstm.h_prev = np.zeros(lstm.hidden_size)\n",
    "    lstm.c_prev = np.zeros(lstm.hidden_size)\n",
    "    x = char_to_onehot(seed_char)\n",
    "    result = [seed_char]\n",
    "    \n",
    "    for _ in range(length):\n",
    "        y_pred, _ = lstm.forward(x)\n",
    "        probs = np.exp(y_pred) / np.sum(np.exp(y_pred))\n",
    "        idx = np.random.choice(len(probs), p=probs.ravel())\n",
    "        char = idx_to_char[idx]\n",
    "        result.append(char)\n",
    "        x = char_to_onehot(char)\n",
    "    return ''.join(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f4096eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = output_size = char_size\n",
    "hidden_size = 50\n",
    "lr = 0.1\n",
    "epochs = 5  # increase later\n",
    "seq_len = len(X_seq)\n",
    "seq_length = 50  # truncated BPTT length\n",
    "\n",
    "# Initialize LSTM\n",
    "lstm = LSTMlatest(input_size, hidden_size, output_size, learning_rate=lr)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     lstm.h_prev = np.zeros(hidden_size)\n",
    "#     lstm.c_prev = np.zeros(hidden_size)\n",
    "#     lstm.caches = []\n",
    "\n",
    "#     outputs = []\n",
    "for i in range(0, len(X_seq) - seq_length, seq_length):\n",
    "    X_batch = X_seq[i:i+seq_length]\n",
    "    Y_batch = Y_seq[i:i+seq_length]\n",
    "    \n",
    "    outputs = []\n",
    "    for t in range(seq_length):\n",
    "        y_pred, _ = lstm.forward(X_batch[t])\n",
    "        outputs.append(y_pred)\n",
    "\n",
    "    # ---------- Forward Pass ----------\n",
    "    for t in range(seq_len):\n",
    "        y_pred, _ = lstm.forward(X_seq[t])\n",
    "        outputs.append(y_pred)\n",
    "\n",
    "    outputs = np.array(outputs)\n",
    "    Y_true = np.array(Y_seq)\n",
    "\n",
    "    # ---------- Compute Loss ----------\n",
    "    exp_scores = np.exp(outputs - np.max(outputs, axis=1, keepdims=True))\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute cross-entropy loss\n",
    "    eps = 1e-8\n",
    "    loss = -np.mean(np.sum(Y_true * np.log(probs + eps), axis=1))\n",
    "\n",
    "    # Gradient of softmax + cross entropy\n",
    "    dY = (probs - Y_true) / output_size\n",
    "\n",
    "    # ---------- Compute initial gradient ----------\n",
    "    # dY = 2 * (outputs - Y_true) / output_size  # dL/dy\n",
    "\n",
    "    dh_next = np.zeros(hidden_size)\n",
    "    dc_next = np.zeros(hidden_size)\n",
    "\n",
    "    # ---------- Backward Pass (BPTT) ----------\n",
    "    for t in reversed(range(seq_len)):\n",
    "        dy = dY[t]\n",
    "        dh_next, dc_next, _ = lstm.backward(dy, dh_next, dc_next)\n",
    "\n",
    "    # ---------- Update ----------\n",
    "    lstm.lr = 0.001  # reduce LR\n",
    "    max_norm = 5\n",
    "\n",
    "    # Before lstm.update()\n",
    "    for dparam in [lstm.dWf, lstm.dWi, lstm.dWo, lstm.dWc, lstm.dbf, lstm.dbi, lstm.dbo, lstm.dbc, lstm.dWy, lstm.dby]:\n",
    "        np.clip(dparam, -max_norm, max_norm, out=dparam)\n",
    "\n",
    "    lstm.update()\n",
    "    lstm.lr *= 0.97\n",
    "\n",
    "    # if epoch % 1 == 0:\n",
    "    print(f'Epoch - Loss: {loss:.6f}')\n",
    "    print(sample(lstm, seed_char='t', length=200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91a850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "uuuinltnltnanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rhlalanlanand\n",
      "\n",
      " rh\n"
     ]
    }
   ],
   "source": [
    "h = np.zeros(hidden_size)\n",
    "c = np.zeros(hidden_size)\n",
    "x = X_seq[0]  # start with first character\n",
    "\n",
    "generated_text = ''\n",
    "for _ in range(200):\n",
    "    y_pred, h = lstm.forward(x)\n",
    "    c = lstm.c_prev\n",
    "    idx = np.argmax(y_pred)  # pick most probable character\n",
    "    generated_text += idx_to_char[idx]\n",
    "    x = np.zeros(char_size)\n",
    "    x[idx] = 1\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd39a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_codes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
